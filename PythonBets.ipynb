{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from  keras.wrappers.scikit_learn  import  KerasClassifier\n",
    "from  sklearn.model_selection  import  GridSearchCV\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53520"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part A)\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/FinalformSora/StockML/main/stocks%20v1%20-%20master%20sheet.csv\")\n",
    "df[0::24]\n",
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Stock name</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close*</th>\n",
       "      <th>Adj Close**</th>\n",
       "      <th>Volume</th>\n",
       "      <th>average volume per stock</th>\n",
       "      <th>Higher or lower then average volume</th>\n",
       "      <th>...</th>\n",
       "      <th>Volitity (1 or 0)</th>\n",
       "      <th>mv50</th>\n",
       "      <th>Higher MV50</th>\n",
       "      <th>Higher than MVA50</th>\n",
       "      <th>MV100</th>\n",
       "      <th>Higher MVA100</th>\n",
       "      <th>Higher than MVA100</th>\n",
       "      <th>EMA 50</th>\n",
       "      <th>Higher then EMVA</th>\n",
       "      <th>Higher than EMVA 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>44294.0</td>\n",
       "      <td>S&amp;P500</td>\n",
       "      <td>4089.95</td>\n",
       "      <td>4098.19</td>\n",
       "      <td>4082.54</td>\n",
       "      <td>4097.17</td>\n",
       "      <td>4097.17</td>\n",
       "      <td>3.901910e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3906.56</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3893.72</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3909.06</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>44293.0</td>\n",
       "      <td>S&amp;P500</td>\n",
       "      <td>4074.29</td>\n",
       "      <td>4083.13</td>\n",
       "      <td>4068.31</td>\n",
       "      <td>4079.95</td>\n",
       "      <td>4079.95</td>\n",
       "      <td>4.112640e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3901.61</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3888.90</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3901.39</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>44292.0</td>\n",
       "      <td>S&amp;P500</td>\n",
       "      <td>4075.57</td>\n",
       "      <td>4086.23</td>\n",
       "      <td>4068.14</td>\n",
       "      <td>4073.94</td>\n",
       "      <td>4073.94</td>\n",
       "      <td>4.027880e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3897.12</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3883.46</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.10</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>44291.0</td>\n",
       "      <td>S&amp;P500</td>\n",
       "      <td>4034.44</td>\n",
       "      <td>4083.42</td>\n",
       "      <td>4034.44</td>\n",
       "      <td>4077.91</td>\n",
       "      <td>4077.91</td>\n",
       "      <td>3.999760e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3892.47</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3878.31</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3886.76</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>44287.0</td>\n",
       "      <td>S&amp;P500</td>\n",
       "      <td>3992.78</td>\n",
       "      <td>4020.63</td>\n",
       "      <td>3992.78</td>\n",
       "      <td>4019.87</td>\n",
       "      <td>4019.87</td>\n",
       "      <td>4.151240e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3887.97</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3873.07</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3878.96</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>44286.0</td>\n",
       "      <td>CokeCola</td>\n",
       "      <td>294.35</td>\n",
       "      <td>297.74</td>\n",
       "      <td>288.59</td>\n",
       "      <td>288.78</td>\n",
       "      <td>288.78</td>\n",
       "      <td>3.410000e+04</td>\n",
       "      <td>4.625314e+04</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>276.98</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>272.16</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>280.08</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2226</td>\n",
       "      <td>44287.0</td>\n",
       "      <td>CokeCola</td>\n",
       "      <td>287.78</td>\n",
       "      <td>289.94</td>\n",
       "      <td>286.95</td>\n",
       "      <td>289.17</td>\n",
       "      <td>289.17</td>\n",
       "      <td>2.280000e+04</td>\n",
       "      <td>4.625314e+04</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>277.64</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>272.73</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>280.43</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2227</td>\n",
       "      <td>44291.0</td>\n",
       "      <td>CokeCola</td>\n",
       "      <td>288.15</td>\n",
       "      <td>298.87</td>\n",
       "      <td>288.00</td>\n",
       "      <td>295.19</td>\n",
       "      <td>295.19</td>\n",
       "      <td>3.000000e+04</td>\n",
       "      <td>4.625314e+04</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>278.39</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>273.27</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>281.01</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2228</td>\n",
       "      <td>44292.0</td>\n",
       "      <td>CokeCola</td>\n",
       "      <td>297.62</td>\n",
       "      <td>301.77</td>\n",
       "      <td>296.14</td>\n",
       "      <td>296.36</td>\n",
       "      <td>296.36</td>\n",
       "      <td>2.140000e+04</td>\n",
       "      <td>4.625314e+04</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>279.07</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>273.86</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>281.61</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2229</td>\n",
       "      <td>44293.0</td>\n",
       "      <td>CokeCola</td>\n",
       "      <td>297.43</td>\n",
       "      <td>299.02</td>\n",
       "      <td>293.63</td>\n",
       "      <td>295.80</td>\n",
       "      <td>295.80</td>\n",
       "      <td>1.900000e+04</td>\n",
       "      <td>4.625314e+04</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>279.65</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>274.39</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>282.17</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2230 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Stock name     Open     High      Low   Close*  Adj Close**  \\\n",
       "0     44294.0     S&P500  4089.95  4098.19  4082.54  4097.17      4097.17   \n",
       "1     44293.0     S&P500  4074.29  4083.13  4068.31  4079.95      4079.95   \n",
       "2     44292.0     S&P500  4075.57  4086.23  4068.14  4073.94      4073.94   \n",
       "3     44291.0     S&P500  4034.44  4083.42  4034.44  4077.91      4077.91   \n",
       "4     44287.0     S&P500  3992.78  4020.63  3992.78  4019.87      4019.87   \n",
       "...       ...        ...      ...      ...      ...      ...          ...   \n",
       "2225  44286.0   CokeCola   294.35   297.74   288.59   288.78       288.78   \n",
       "2226  44287.0   CokeCola   287.78   289.94   286.95   289.17       289.17   \n",
       "2227  44291.0   CokeCola   288.15   298.87   288.00   295.19       295.19   \n",
       "2228  44292.0   CokeCola   297.62   301.77   296.14   296.36       296.36   \n",
       "2229  44293.0   CokeCola   297.43   299.02   293.63   295.80       295.80   \n",
       "\n",
       "            Volume  average volume per stock  \\\n",
       "0     3.901910e+09              4.358682e+09   \n",
       "1     4.112640e+09              4.358682e+09   \n",
       "2     4.027880e+09              4.358682e+09   \n",
       "3     3.999760e+09              4.358682e+09   \n",
       "4     4.151240e+09              4.358682e+09   \n",
       "...            ...                       ...   \n",
       "2225  3.410000e+04              4.625314e+04   \n",
       "2226  2.280000e+04              4.625314e+04   \n",
       "2227  3.000000e+04              4.625314e+04   \n",
       "2228  2.140000e+04              4.625314e+04   \n",
       "2229  1.900000e+04              4.625314e+04   \n",
       "\n",
       "     Higher or lower then average volume  ...  Volitity (1 or 0)     mv50  \\\n",
       "0                                     No  ...                  0  3906.56   \n",
       "1                                     No  ...                  0  3901.61   \n",
       "2                                     No  ...                  0  3897.12   \n",
       "3                                     No  ...                  0  3892.47   \n",
       "4                                     No  ...                  0  3887.97   \n",
       "...                                  ...  ...                ...      ...   \n",
       "2225                                  No  ...                  0   276.98   \n",
       "2226                                  No  ...                  0   277.64   \n",
       "2227                                  No  ...                  0   278.39   \n",
       "2228                                  No  ...                  0   279.07   \n",
       "2229                                  No  ...                  0   279.65   \n",
       "\n",
       "      Higher MV50 Higher than MVA50    MV100  Higher MVA100  \\\n",
       "0             Yes                 1  3893.72            Yes   \n",
       "1             Yes                 1  3888.90            Yes   \n",
       "2             Yes                 1  3883.46            Yes   \n",
       "3             Yes                 1  3878.31            Yes   \n",
       "4             Yes                 1  3873.07            Yes   \n",
       "...           ...               ...      ...            ...   \n",
       "2225          Yes                 1   272.16            Yes   \n",
       "2226          Yes                 1   272.73            Yes   \n",
       "2227          Yes                 1   273.27            Yes   \n",
       "2228          Yes                 1   273.86            Yes   \n",
       "2229          Yes                 1   274.39            Yes   \n",
       "\n",
       "     Higher than MVA100   EMA 50  Higher then EMVA Higher than EMVA 1  \n",
       "0                     1  3909.06                No                  0  \n",
       "1                     1  3901.39                No                  0  \n",
       "2                     1  3894.10                No                  0  \n",
       "3                     1  3886.76                No                  0  \n",
       "4                     1  3878.96                No                  0  \n",
       "...                 ...      ...               ...                ...  \n",
       "2225                  1   280.08                No                  0  \n",
       "2226                  1   280.43                No                  0  \n",
       "2227                  1   281.01                No                  0  \n",
       "2228                  1   281.61                No                  0  \n",
       "2229                  1   282.17                No                  0  \n",
       "\n",
       "[2230 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data that  has been parsed and cleaned up\n",
    "#Had to create a moving average for 50 day and 100 day. It requres calculation of previous days and finding the average\n",
    "#increase is based on CLose-Open= if it equals positive its a true on increase\n",
    "#AMD news is based on press release released by amd on their site. I had to manual add this in cause no site has this information parsed\n",
    "#quarterly earning is also gather by searching the web for this release date\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Stock name</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close*</th>\n",
       "      <th>Adj Close**</th>\n",
       "      <th>Volume</th>\n",
       "      <th>average volume per stock</th>\n",
       "      <th>Higher or lower then average volume</th>\n",
       "      <th>...</th>\n",
       "      <th>Volitity (1 or 0)</th>\n",
       "      <th>mv50</th>\n",
       "      <th>Higher MV50</th>\n",
       "      <th>Higher than MVA50</th>\n",
       "      <th>MV100</th>\n",
       "      <th>Higher MVA100</th>\n",
       "      <th>Higher than MVA100</th>\n",
       "      <th>EMA 50</th>\n",
       "      <th>Higher then EMVA</th>\n",
       "      <th>Higher than EMVA 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>44294.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4089.95</td>\n",
       "      <td>4098.19</td>\n",
       "      <td>4082.54</td>\n",
       "      <td>4097.17</td>\n",
       "      <td>4097.17</td>\n",
       "      <td>3.901910e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3906.56</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3893.72</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3909.06</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>44293.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4074.29</td>\n",
       "      <td>4083.13</td>\n",
       "      <td>4068.31</td>\n",
       "      <td>4079.95</td>\n",
       "      <td>4079.95</td>\n",
       "      <td>4.112640e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3901.61</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3888.90</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3901.39</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>44292.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4075.57</td>\n",
       "      <td>4086.23</td>\n",
       "      <td>4068.14</td>\n",
       "      <td>4073.94</td>\n",
       "      <td>4073.94</td>\n",
       "      <td>4.027880e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3897.12</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3883.46</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.10</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>44291.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4034.44</td>\n",
       "      <td>4083.42</td>\n",
       "      <td>4034.44</td>\n",
       "      <td>4077.91</td>\n",
       "      <td>4077.91</td>\n",
       "      <td>3.999760e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3892.47</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3878.31</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3886.76</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>44287.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3992.78</td>\n",
       "      <td>4020.63</td>\n",
       "      <td>3992.78</td>\n",
       "      <td>4019.87</td>\n",
       "      <td>4019.87</td>\n",
       "      <td>4.151240e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3887.97</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3873.07</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3878.96</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date  Stock name     Open     High      Low   Close*  Adj Close**  \\\n",
       "0  44294.0           2  4089.95  4098.19  4082.54  4097.17      4097.17   \n",
       "1  44293.0           2  4074.29  4083.13  4068.31  4079.95      4079.95   \n",
       "2  44292.0           2  4075.57  4086.23  4068.14  4073.94      4073.94   \n",
       "3  44291.0           2  4034.44  4083.42  4034.44  4077.91      4077.91   \n",
       "4  44287.0           2  3992.78  4020.63  3992.78  4019.87      4019.87   \n",
       "\n",
       "         Volume  average volume per stock Higher or lower then average volume  \\\n",
       "0  3.901910e+09              4.358682e+09                                  No   \n",
       "1  4.112640e+09              4.358682e+09                                  No   \n",
       "2  4.027880e+09              4.358682e+09                                  No   \n",
       "3  3.999760e+09              4.358682e+09                                  No   \n",
       "4  4.151240e+09              4.358682e+09                                  No   \n",
       "\n",
       "   ...  Volitity (1 or 0)     mv50  Higher MV50 Higher than MVA50    MV100  \\\n",
       "0  ...                  0  3906.56          Yes                 1  3893.72   \n",
       "1  ...                  0  3901.61          Yes                 1  3888.90   \n",
       "2  ...                  0  3897.12          Yes                 1  3883.46   \n",
       "3  ...                  0  3892.47          Yes                 1  3878.31   \n",
       "4  ...                  0  3887.97          Yes                 1  3873.07   \n",
       "\n",
       "   Higher MVA100 Higher than MVA100   EMA 50  Higher then EMVA  \\\n",
       "0            Yes                  1  3909.06                No   \n",
       "1            Yes                  1  3901.39                No   \n",
       "2            Yes                  1  3894.10                No   \n",
       "3            Yes                  1  3886.76                No   \n",
       "4            Yes                  1  3878.96                No   \n",
       "\n",
       "  Higher than EMVA 1  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_StockName = {\"Stock name\": {\"CokeCola\": 1, \"S&P500\": 2,\"Apple\": 3,\"Gamestop\":4}}\n",
    "df = df.replace(cleanup_StockName)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Stock name</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close*</th>\n",
       "      <th>Adj Close**</th>\n",
       "      <th>Volume</th>\n",
       "      <th>average volume per stock</th>\n",
       "      <th>Higher or lower then average volume</th>\n",
       "      <th>...</th>\n",
       "      <th>Volitity (1 or 0)</th>\n",
       "      <th>mv50</th>\n",
       "      <th>Higher MV50</th>\n",
       "      <th>Higher than MVA50</th>\n",
       "      <th>MV100</th>\n",
       "      <th>Higher MVA100</th>\n",
       "      <th>Higher than MVA100</th>\n",
       "      <th>EMA 50</th>\n",
       "      <th>Higher then EMVA</th>\n",
       "      <th>Higher than EMVA 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>44294.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4089.95</td>\n",
       "      <td>4098.19</td>\n",
       "      <td>4082.54</td>\n",
       "      <td>4097.17</td>\n",
       "      <td>4097.17</td>\n",
       "      <td>3.901910e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3906.56</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3893.72</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3909.06</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>44293.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4074.29</td>\n",
       "      <td>4083.13</td>\n",
       "      <td>4068.31</td>\n",
       "      <td>4079.95</td>\n",
       "      <td>4079.95</td>\n",
       "      <td>4.112640e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3901.61</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3888.90</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3901.39</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>44292.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4075.57</td>\n",
       "      <td>4086.23</td>\n",
       "      <td>4068.14</td>\n",
       "      <td>4073.94</td>\n",
       "      <td>4073.94</td>\n",
       "      <td>4.027880e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3897.12</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3883.46</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.10</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>44291.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4034.44</td>\n",
       "      <td>4083.42</td>\n",
       "      <td>4034.44</td>\n",
       "      <td>4077.91</td>\n",
       "      <td>4077.91</td>\n",
       "      <td>3.999760e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3892.47</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3878.31</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3886.76</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>44287.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3992.78</td>\n",
       "      <td>4020.63</td>\n",
       "      <td>3992.78</td>\n",
       "      <td>4019.87</td>\n",
       "      <td>4019.87</td>\n",
       "      <td>4.151240e+09</td>\n",
       "      <td>4.358682e+09</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3887.97</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3873.07</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3878.96</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>44286.0</td>\n",
       "      <td>1</td>\n",
       "      <td>294.35</td>\n",
       "      <td>297.74</td>\n",
       "      <td>288.59</td>\n",
       "      <td>288.78</td>\n",
       "      <td>288.78</td>\n",
       "      <td>3.410000e+04</td>\n",
       "      <td>4.625314e+04</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>276.98</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>272.16</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>280.08</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2226</td>\n",
       "      <td>44287.0</td>\n",
       "      <td>1</td>\n",
       "      <td>287.78</td>\n",
       "      <td>289.94</td>\n",
       "      <td>286.95</td>\n",
       "      <td>289.17</td>\n",
       "      <td>289.17</td>\n",
       "      <td>2.280000e+04</td>\n",
       "      <td>4.625314e+04</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>277.64</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>272.73</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>280.43</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2227</td>\n",
       "      <td>44291.0</td>\n",
       "      <td>1</td>\n",
       "      <td>288.15</td>\n",
       "      <td>298.87</td>\n",
       "      <td>288.00</td>\n",
       "      <td>295.19</td>\n",
       "      <td>295.19</td>\n",
       "      <td>3.000000e+04</td>\n",
       "      <td>4.625314e+04</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>278.39</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>273.27</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>281.01</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2228</td>\n",
       "      <td>44292.0</td>\n",
       "      <td>1</td>\n",
       "      <td>297.62</td>\n",
       "      <td>301.77</td>\n",
       "      <td>296.14</td>\n",
       "      <td>296.36</td>\n",
       "      <td>296.36</td>\n",
       "      <td>2.140000e+04</td>\n",
       "      <td>4.625314e+04</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>279.07</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>273.86</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>281.61</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2229</td>\n",
       "      <td>44293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>297.43</td>\n",
       "      <td>299.02</td>\n",
       "      <td>293.63</td>\n",
       "      <td>295.80</td>\n",
       "      <td>295.80</td>\n",
       "      <td>1.900000e+04</td>\n",
       "      <td>4.625314e+04</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>279.65</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>274.39</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>282.17</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2230 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Stock name     Open     High      Low   Close*  Adj Close**  \\\n",
       "0     44294.0           2  4089.95  4098.19  4082.54  4097.17      4097.17   \n",
       "1     44293.0           2  4074.29  4083.13  4068.31  4079.95      4079.95   \n",
       "2     44292.0           2  4075.57  4086.23  4068.14  4073.94      4073.94   \n",
       "3     44291.0           2  4034.44  4083.42  4034.44  4077.91      4077.91   \n",
       "4     44287.0           2  3992.78  4020.63  3992.78  4019.87      4019.87   \n",
       "...       ...         ...      ...      ...      ...      ...          ...   \n",
       "2225  44286.0           1   294.35   297.74   288.59   288.78       288.78   \n",
       "2226  44287.0           1   287.78   289.94   286.95   289.17       289.17   \n",
       "2227  44291.0           1   288.15   298.87   288.00   295.19       295.19   \n",
       "2228  44292.0           1   297.62   301.77   296.14   296.36       296.36   \n",
       "2229  44293.0           1   297.43   299.02   293.63   295.80       295.80   \n",
       "\n",
       "            Volume  average volume per stock  \\\n",
       "0     3.901910e+09              4.358682e+09   \n",
       "1     4.112640e+09              4.358682e+09   \n",
       "2     4.027880e+09              4.358682e+09   \n",
       "3     3.999760e+09              4.358682e+09   \n",
       "4     4.151240e+09              4.358682e+09   \n",
       "...            ...                       ...   \n",
       "2225  3.410000e+04              4.625314e+04   \n",
       "2226  2.280000e+04              4.625314e+04   \n",
       "2227  3.000000e+04              4.625314e+04   \n",
       "2228  2.140000e+04              4.625314e+04   \n",
       "2229  1.900000e+04              4.625314e+04   \n",
       "\n",
       "     Higher or lower then average volume  ...  Volitity (1 or 0)     mv50  \\\n",
       "0                                     No  ...                  0  3906.56   \n",
       "1                                     No  ...                  0  3901.61   \n",
       "2                                     No  ...                  0  3897.12   \n",
       "3                                     No  ...                  0  3892.47   \n",
       "4                                     No  ...                  0  3887.97   \n",
       "...                                  ...  ...                ...      ...   \n",
       "2225                                  No  ...                  0   276.98   \n",
       "2226                                  No  ...                  0   277.64   \n",
       "2227                                  No  ...                  0   278.39   \n",
       "2228                                  No  ...                  0   279.07   \n",
       "2229                                  No  ...                  0   279.65   \n",
       "\n",
       "      Higher MV50 Higher than MVA50    MV100  Higher MVA100  \\\n",
       "0             Yes                 1  3893.72            Yes   \n",
       "1             Yes                 1  3888.90            Yes   \n",
       "2             Yes                 1  3883.46            Yes   \n",
       "3             Yes                 1  3878.31            Yes   \n",
       "4             Yes                 1  3873.07            Yes   \n",
       "...           ...               ...      ...            ...   \n",
       "2225          Yes                 1   272.16            Yes   \n",
       "2226          Yes                 1   272.73            Yes   \n",
       "2227          Yes                 1   273.27            Yes   \n",
       "2228          Yes                 1   273.86            Yes   \n",
       "2229          Yes                 1   274.39            Yes   \n",
       "\n",
       "     Higher than MVA100   EMA 50  Higher then EMVA Higher than EMVA 1  \n",
       "0                     1  3909.06                No                  0  \n",
       "1                     1  3901.39                No                  0  \n",
       "2                     1  3894.10                No                  0  \n",
       "3                     1  3886.76                No                  0  \n",
       "4                     1  3878.96                No                  0  \n",
       "...                 ...      ...               ...                ...  \n",
       "2225                  1   280.08                No                  0  \n",
       "2226                  1   280.43                No                  0  \n",
       "2227                  1   281.01                No                  0  \n",
       "2228                  1   281.61                No                  0  \n",
       "2229                  1   282.17                No                  0  \n",
       "\n",
       "[2230 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock name</th>\n",
       "      <th>Volume avg</th>\n",
       "      <th>Volitity (1 or 0)</th>\n",
       "      <th>Higher than MVA50</th>\n",
       "      <th>Higher than MVA100</th>\n",
       "      <th>Higher than EMVA 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stock name  Volume avg  Volitity (1 or 0)  Higher than MVA50  \\\n",
       "0           2           0                  0                  1   \n",
       "1           2           0                  0                  1   \n",
       "2           2           0                  0                  1   \n",
       "3           2           0                  0                  1   \n",
       "4           2           0                  0                  1   \n",
       "\n",
       "   Higher than MVA100  Higher than EMVA 1  \n",
       "0                   1                   0  \n",
       "1                   1                   0  \n",
       "2                   1                   0  \n",
       "3                   1                   0  \n",
       "4                   1                   0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = ['Stock name','Volume avg','Volitity (1 or 0)','Higher than MVA50','Higher than MVA100','Higher than EMVA 1',]\n",
    "X = df[feature_cols]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created the label based on if the stock increased or did not increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "25      0\n",
       "50      0\n",
       "75      1\n",
       "100     1\n",
       "       ..\n",
       "2125    0\n",
       "2150    1\n",
       "2175    0\n",
       "2200    1\n",
       "2225    0\n",
       "Name: higher closing 0 or 1, Length: 90, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['higher closing 0 or 1']\n",
    "y[::25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling all the classifers\n",
    "my_logreg = LogisticRegression()\n",
    "my_decisiontree = DecisionTreeClassifier()\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "my_RandomForest = RandomForestClassifier()\n",
    "my_linreg = LinearRegression()\n",
    "my_AdaBoost = AdaBoostClassifier(n_estimators = 29,random_state=2)\n",
    "my_XGBoost = XGBClassifier(n_estimators = 29,random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seting up train set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:19:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=29, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=2, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=True,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training sets\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "my_logreg.fit(X_train, y_train)\n",
    "my_RandomForest.fit(X_train, y_train)\n",
    "my_decisiontree.fit(X_train, y_train)\n",
    "my_linreg.fit(X_train, y_train)\n",
    "my_AdaBoost.fit(X_train,y_train)\n",
    "my_XGBoost.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "y_predict_lor = my_logreg.predict(X_test)\n",
    "y_predict = knn.predict(X_test)\n",
    "y_predict_dt = my_decisiontree.predict(X_test)\n",
    "y_predict_rt = my_RandomForest.predict(X_test)\n",
    "y_predict_LR = my_linreg.predict(X_test)\n",
    "y_predict_AB = my_AdaBoost.predict(X_test)\n",
    "y_predict_XB = my_XGBoost.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression 0.49425579970215155\n",
      "LogisticRegression:  0.5770609318996416\n",
      "Decision Tree:  0.5698924731182796\n",
      "Random Forest:  0.5734767025089605\n",
      "knn:  0.4767025089605735\n",
      "AdaBoost:  0.578853046594982\n",
      "XGBoost:  0.5716845878136201\n"
     ]
    }
   ],
   "source": [
    "#accuracy of the perdiction\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_lor = accuracy_score(y_test, y_predict_lor)\n",
    "score_dt = accuracy_score(y_test, y_predict_dt)\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "score_rt = accuracy_score(y_test,y_predict_rt)\n",
    "score_AB = accuracy_score(y_test, y_predict_AB)\n",
    "score_XB = accuracy_score(y_test, y_predict_XB)\n",
    "\n",
    "y_predict_LR = my_linreg.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test, y_predict_LR)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Linear Regression\",rmse)\n",
    "print('LogisticRegression: ',score_lor)\n",
    "print(\"Decision Tree: \",score_dt)\n",
    "print(\"Random Forest: \",score_rt)\n",
    "print(\"knn: \",accuracy)\n",
    "print(\"AdaBoost: \",score_AB)\n",
    "print(\"XGBoost: \",score_XB)\n",
    "\n",
    "#Linear Regression doesnt have Predict Proba\n",
    "var_LogR = my_logreg.predict_proba(X_test)\n",
    "fpr_LogR, tpr_LogR, thresholds = metrics.roc_curve(y_test, var_LogR[:,1], pos_label=1)\n",
    "AUC_LogR = metrics.auc(fpr_LogR, tpr_LogR)\n",
    "\n",
    "var_DT = my_decisiontree.predict_proba(X_test)\n",
    "fpr_DT, tpr_DT, thresholds = metrics.roc_curve(y_test, var_DT[:,1], pos_label=1)\n",
    "AUC_dt = metrics.auc(fpr_DT, tpr_DT)\n",
    "\n",
    "var_RF = my_RandomForest.predict_proba(X_test)\n",
    "fpr_RF, tpr_RF, thresholds = metrics.roc_curve(y_test, var_RF[:,1], pos_label=1)\n",
    "AUC_RF = metrics.auc(fpr_RF, tpr_RF)\n",
    "\n",
    "var_KNN = knn.predict_proba(X_test)\n",
    "fpr_KNN, tpr_KNN, thresholds = metrics.roc_curve(y_test, var_KNN[:,1], pos_label=1)\n",
    "AUC_KNN = metrics.auc(fpr_KNN, tpr_KNN)\n",
    "\n",
    "var_AB = my_AdaBoost.predict_proba(X_test)\n",
    "fpr_AB, tpr_AB, thresholds = metrics.roc_curve(y_test, var_AB[:,1], pos_label=1)\n",
    "AUC_AB = metrics.auc(fpr_AB, tpr_AB)\n",
    "\n",
    "var_XB = my_XGBoost.predict_proba(X_test)\n",
    "fpr_XB, tpr_XB, thresholds = metrics.roc_curve(y_test, var_XB[:,1], pos_label=1)\n",
    "AUC_XB = metrics.auc(fpr_XB, tpr_XB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#8) plot the ROC curves of your algorithms in parts (c), (d), (e), (f), (g) \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Roc Curve1:\n",
    "plt.plot(fpr_DT, tpr_DT, color='red', lw=3, \n",
    "         label='ROC Curve DecisionTree' % AUC_dt)\n",
    "\n",
    "# Roc Curve2:\n",
    "plt.plot(fpr_LogR, tpr_LogR, color='pink', lw=3, \n",
    "         label='ROC Curve Logistic Regression' % AUC_LogR)\n",
    "\n",
    "# Roc Curve3:\n",
    "plt.plot(fpr_AB, tpr_AB, color='yellow', lw=2, \n",
    "         label='ROC Curve AdaBoost' % AUC_AB)\n",
    "\n",
    "# Roc Curve4:\n",
    "plt.plot(fpr_XB, tpr_XB, color='cyan', lw=2, \n",
    "         label='ROC Curve XGBoost' % AUC_XB)\n",
    "\n",
    "plt.plot(fpr_RF, tpr_RF, color='magenta', lw=2, \n",
    "         label='ROC Curve RandomForest' % AUC_RF)\n",
    "plt.plot(fpr_RF, tpr_RF, color='magenta', lw=2, \n",
    "         label='ROC Curve RandomForest' % AUC_RF)\n",
    "\n",
    "\n",
    "fpr_RF\n",
    "# Random Guess line:\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
    "\n",
    "# Defining The Range of X-Axis and Y-Axis:\n",
    "plt.xlim([-0.005, 1.005])\n",
    "plt.ylim([0.0, 1.01])\n",
    "\n",
    "# Labels, Title, Legend:\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation of all Classifers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.4932553088803088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = cross_val_score(my_logreg, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"Logistic Regression: \",accuracy_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:  0.41352155727155726\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = cross_val_score(my_RandomForest, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "\n",
    "print(\"Random Forest: \",accuracy_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:  0.41886261261261265\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "\n",
    "print(\"KNN: \",accuracy_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decisiontree:  0.4309684684684685\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = cross_val_score(my_decisiontree, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "\n",
    "print(\"decisiontree: \",accuracy_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:  0.49712753948225075\n"
     ]
    }
   ],
   "source": [
    "mse_list = cross_val_score(my_linreg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_list_positive = -mse_list\n",
    "rmse_list = np.sqrt(mse_list_positive)\n",
    "print(\"Linear Regression: \",rmse_list.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost :  0.46760376447876445\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = cross_val_score(my_AdaBoost, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_ab = accuracy_list.mean()\n",
    "\n",
    "print(\"Adaboost : \",accuracy_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:19:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:19:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:19:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:19:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:19:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:19:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20:19:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:19:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:19:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:19:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost:  0.4296171171171171\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = cross_val_score(my_XGBoost, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_XG = accuracy_list.mean()\n",
    "\n",
    "print(\"XGBoost: \",accuracy_XG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44737412, -0.68486364, -1.        ,  0.73054783,  0.72406161,\n",
       "        -0.7018812 ],\n",
       "       [-0.44737412, -0.68486364, -1.        ,  0.73054783,  0.72406161,\n",
       "        -0.7018812 ],\n",
       "       [-0.44737412, -0.68486364, -1.        ,  0.73054783,  0.72406161,\n",
       "        -0.7018812 ],\n",
       "       ...,\n",
       "       [-1.34212235, -0.68486364, -1.        ,  0.73054783,  0.72406161,\n",
       "        -0.7018812 ],\n",
       "       [-1.34212235, -0.68486364, -1.        ,  0.73054783,  0.72406161,\n",
       "        -0.7018812 ],\n",
       "       [-1.34212235, -0.68486364, -1.        ,  0.73054783,  0.72406161,\n",
       "        -0.7018812 ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "preprocessing_features = preprocessing.scale(X)\n",
    "preprocessing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69664251\n",
      "Iteration 2, loss = 0.69015354\n",
      "Iteration 3, loss = 0.69045916\n",
      "Iteration 4, loss = 0.70294732\n",
      "Iteration 5, loss = 0.69049225\n",
      "Iteration 6, loss = 0.69189355\n",
      "Iteration 7, loss = 0.68874607\n",
      "Iteration 8, loss = 0.69137206\n",
      "Iteration 9, loss = 0.68842802\n",
      "Iteration 10, loss = 0.69260204\n",
      "Iteration 11, loss = 0.69946004\n",
      "Iteration 12, loss = 0.68825308\n",
      "Iteration 13, loss = 0.68879131\n",
      "Iteration 14, loss = 0.69588369\n",
      "Iteration 15, loss = 0.69164209\n",
      "Iteration 16, loss = 0.69270426\n",
      "Iteration 17, loss = 0.69311327\n",
      "Iteration 18, loss = 0.69042326\n",
      "Iteration 19, loss = 0.69003976\n",
      "Iteration 20, loss = 0.69060657\n",
      "Iteration 21, loss = 0.68952447\n",
      "Iteration 22, loss = 0.69472355\n",
      "Iteration 23, loss = 0.69384379\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69746371\n",
      "Iteration 2, loss = 0.69255591\n",
      "Iteration 3, loss = 0.69345499\n",
      "Iteration 4, loss = 0.70385583\n",
      "Iteration 5, loss = 0.68927348\n",
      "Iteration 6, loss = 0.69162154\n",
      "Iteration 7, loss = 0.68934425\n",
      "Iteration 8, loss = 0.68988281\n",
      "Iteration 9, loss = 0.68975910\n",
      "Iteration 10, loss = 0.69859555\n",
      "Iteration 11, loss = 0.70012802\n",
      "Iteration 12, loss = 0.68970423\n",
      "Iteration 13, loss = 0.68958045\n",
      "Iteration 14, loss = 0.69795432\n",
      "Iteration 15, loss = 0.69197658\n",
      "Iteration 16, loss = 0.69709109\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69599016\n",
      "Iteration 2, loss = 0.68723419\n",
      "Iteration 3, loss = 0.68906749\n",
      "Iteration 4, loss = 0.71303176\n",
      "Iteration 5, loss = 0.68796905\n",
      "Iteration 6, loss = 0.68742101\n",
      "Iteration 7, loss = 0.68604293\n",
      "Iteration 8, loss = 0.68858022\n",
      "Iteration 9, loss = 0.68692436\n",
      "Iteration 10, loss = 0.68880714\n",
      "Iteration 11, loss = 0.68890277\n",
      "Iteration 12, loss = 0.68634406\n",
      "Iteration 13, loss = 0.68652374\n",
      "Iteration 14, loss = 0.69571891\n",
      "Iteration 15, loss = 0.68963159\n",
      "Iteration 16, loss = 0.69037334\n",
      "Iteration 17, loss = 0.69229666\n",
      "Iteration 18, loss = 0.68822177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69959678\n",
      "Iteration 2, loss = 0.68858111\n",
      "Iteration 3, loss = 0.69090438\n",
      "Iteration 4, loss = 0.70400143\n",
      "Iteration 5, loss = 0.68964703\n",
      "Iteration 6, loss = 0.69053057\n",
      "Iteration 7, loss = 0.68829657\n",
      "Iteration 8, loss = 0.68941666\n",
      "Iteration 9, loss = 0.68854812\n",
      "Iteration 10, loss = 0.69043378\n",
      "Iteration 11, loss = 0.68953103\n",
      "Iteration 12, loss = 0.69056628\n",
      "Iteration 13, loss = 0.69523738\n",
      "Iteration 14, loss = 0.68922931\n",
      "Iteration 15, loss = 0.69149087\n",
      "Iteration 16, loss = 0.69265269\n",
      "Iteration 17, loss = 0.69028524\n",
      "Iteration 18, loss = 0.68976419\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69992326\n",
      "Iteration 2, loss = 0.69060238\n",
      "Iteration 3, loss = 0.69828797\n",
      "Iteration 4, loss = 0.69977028\n",
      "Iteration 5, loss = 0.69280486\n",
      "Iteration 6, loss = 0.69682744\n",
      "Iteration 7, loss = 0.69678216\n",
      "Iteration 8, loss = 0.69234857\n",
      "Iteration 9, loss = 0.69103632\n",
      "Iteration 10, loss = 0.69240376\n",
      "Iteration 11, loss = 0.69156626\n",
      "Iteration 12, loss = 0.69290493\n",
      "Iteration 13, loss = 0.69160678\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69846167\n",
      "Iteration 2, loss = 0.70226471\n",
      "Iteration 3, loss = 0.69013500\n",
      "Iteration 4, loss = 0.68812110\n",
      "Iteration 5, loss = 0.68899623\n",
      "Iteration 6, loss = 0.68787745\n",
      "Iteration 7, loss = 0.68686837\n",
      "Iteration 8, loss = 0.69684378\n",
      "Iteration 9, loss = 0.68877369\n",
      "Iteration 10, loss = 0.69064789\n",
      "Iteration 11, loss = 0.69672713\n",
      "Iteration 12, loss = 0.69392429\n",
      "Iteration 13, loss = 0.68790598\n",
      "Iteration 14, loss = 0.68974353\n",
      "Iteration 15, loss = 0.69021444\n",
      "Iteration 16, loss = 0.69382924\n",
      "Iteration 17, loss = 0.68919710\n",
      "Iteration 18, loss = 0.69599169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69651972\n",
      "Iteration 2, loss = 0.69675022\n",
      "Iteration 3, loss = 0.68723778\n",
      "Iteration 4, loss = 0.68622473\n",
      "Iteration 5, loss = 0.68488821\n",
      "Iteration 6, loss = 0.68583100\n",
      "Iteration 7, loss = 0.68561833\n",
      "Iteration 8, loss = 0.68918011\n",
      "Iteration 9, loss = 0.68815065\n",
      "Iteration 10, loss = 0.69341827\n",
      "Iteration 11, loss = 0.68996024\n",
      "Iteration 12, loss = 0.69366600\n",
      "Iteration 13, loss = 0.68823216\n",
      "Iteration 14, loss = 0.69031173\n",
      "Iteration 15, loss = 0.68709220\n",
      "Iteration 16, loss = 0.69122007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70741973\n",
      "Iteration 2, loss = 0.69117062\n",
      "Iteration 3, loss = 0.68806707\n",
      "Iteration 4, loss = 0.69455630\n",
      "Iteration 5, loss = 0.68793154\n",
      "Iteration 6, loss = 0.69086875\n",
      "Iteration 7, loss = 0.68798672\n",
      "Iteration 8, loss = 0.69140684\n",
      "Iteration 9, loss = 0.68822245\n",
      "Iteration 10, loss = 0.69549828\n",
      "Iteration 11, loss = 0.69475832\n",
      "Iteration 12, loss = 0.69555114\n",
      "Iteration 13, loss = 0.69336139\n",
      "Iteration 14, loss = 0.69690814\n",
      "Iteration 15, loss = 0.69464277\n",
      "Iteration 16, loss = 0.68859419\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70215473\n",
      "Iteration 2, loss = 0.69480126\n",
      "Iteration 3, loss = 0.69414591\n",
      "Iteration 4, loss = 0.70063989\n",
      "Iteration 5, loss = 0.68940275\n",
      "Iteration 6, loss = 0.69298960\n",
      "Iteration 7, loss = 0.68982070\n",
      "Iteration 8, loss = 0.68930434\n",
      "Iteration 9, loss = 0.68985955\n",
      "Iteration 10, loss = 0.69735949\n",
      "Iteration 11, loss = 0.69668701\n",
      "Iteration 12, loss = 0.70080414\n",
      "Iteration 13, loss = 0.69657208\n",
      "Iteration 14, loss = 0.70036356\n",
      "Iteration 15, loss = 0.70423812\n",
      "Iteration 16, loss = 0.69234744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69768055\n",
      "Iteration 2, loss = 0.69683726\n",
      "Iteration 3, loss = 0.69056092\n",
      "Iteration 4, loss = 0.69298226\n",
      "Iteration 5, loss = 0.69292937\n",
      "Iteration 6, loss = 0.68999724\n",
      "Iteration 7, loss = 0.69063171\n",
      "Iteration 8, loss = 0.68966109\n",
      "Iteration 9, loss = 0.68946920\n",
      "Iteration 10, loss = 0.70297482\n",
      "Iteration 11, loss = 0.69517467\n",
      "Iteration 12, loss = 0.69584016\n",
      "Iteration 13, loss = 0.69161687\n",
      "Iteration 14, loss = 0.69625179\n",
      "Iteration 15, loss = 0.70111983\n",
      "Iteration 16, loss = 0.69235976\n",
      "Iteration 17, loss = 0.69480859\n",
      "Iteration 18, loss = 0.68992312\n",
      "Iteration 19, loss = 0.68912446\n",
      "Iteration 20, loss = 0.68984008\n",
      "Iteration 21, loss = 0.69016014\n",
      "Iteration 22, loss = 0.69078140\n",
      "Iteration 23, loss = 0.69036808\n",
      "Iteration 24, loss = 0.68995841\n",
      "Iteration 25, loss = 0.69292384\n",
      "Iteration 26, loss = 0.69044501\n",
      "Iteration 27, loss = 0.68908066\n",
      "Iteration 28, loss = 0.69204091\n",
      "Iteration 29, loss = 0.69805093\n",
      "Iteration 30, loss = 0.70313527\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "my_ANN = MLPClassifier(hidden_layer_sizes=(30,), activation= 'logistic', \n",
    "                       solver='adam', alpha=1, random_state=1, \n",
    "                       learning_rate_init = 0.02, verbose=True)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracy_list = cross_val_score(my_ANN, preprocessing_features, y, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49673825611325617\n",
      "Accuracy for ML is 49.67382561132562%\n"
     ]
    }
   ],
   "source": [
    "accuracy_cv_mean = accuracy_list.mean()\n",
    "\n",
    "print(accuracy_cv_mean)\n",
    "print(\"Accuracy for ML is \" + str((accuracy_cv_mean * 100)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:06:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Linear Regression 0.494128924757012\n",
      "LogisticRegression:  0.5806451612903226\n",
      "Decision Tree:  0.5645161290322581\n",
      "Random Forest:  0.5645161290322581\n",
      "knn:  0.48028673835125446\n",
      "AdaBoost:  0.5752688172043011\n",
      "XGBoost:  0.5645161290322581\n",
      "Linear Regression:  0.49691967989592917\n",
      "Logistic Regression:  0.5098375160875162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decisiontree:  0.4175233268983269\n",
      "Random Forest:  0.43045768983268984\n",
      "KNN:  0.44970640283140284\n",
      "Adaboost :  0.4751890283140283\n",
      "[21:06:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21:06:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:06:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost:  0.41484475546975547\n"
     ]
    }
   ],
   "source": [
    "#feature_cols = ['Stock name','Volume avg','Volitity (1 or 0)','Higher than MVA50','Higher than MVA100','Higher than MVA100','Higher than EMVA 1']\n",
    "feature_cols_No_EMVA = ['Stock name','Volume avg','Volitity (1 or 0)','Higher than MVA50','Higher than MVA100',]\n",
    "X = df[feature_cols_No_EMVA]\n",
    "y = df['higher closing 0 or 1']\n",
    "\n",
    "#seting up train set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "my_logreg.fit(X_train, y_train)\n",
    "my_RandomForest.fit(X_train, y_train)\n",
    "my_decisiontree.fit(X_train, y_train)\n",
    "my_linreg.fit(X_train, y_train)\n",
    "my_AdaBoost.fit(X_train,y_train)\n",
    "my_XGBoost.fit(X_train,y_train)\n",
    "\n",
    "#Training\n",
    "y_predict_lor = my_logreg.predict(X_test)\n",
    "y_predict = knn.predict(X_test)\n",
    "y_predict_dt = my_decisiontree.predict(X_test)\n",
    "y_predict_rt = my_RandomForest.predict(X_test)\n",
    "y_predict_LR = my_linreg.predict(X_test)\n",
    "y_predict_AB = my_AdaBoost.predict(X_test)\n",
    "y_predict_XB = my_XGBoost.predict(X_test)\n",
    "\n",
    "#accuracy of the perdiction\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_lor = accuracy_score(y_test, y_predict_lor)\n",
    "score_dt = accuracy_score(y_test, y_predict_dt)\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "score_rt = accuracy_score(y_test,y_predict_rt)\n",
    "score_AB = accuracy_score(y_test, y_predict_AB)\n",
    "score_XB = accuracy_score(y_test, y_predict_XB)\n",
    "\n",
    "y_predict_LR = my_linreg.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test, y_predict_LR)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Linear Regression\",rmse)\n",
    "print('LogisticRegression: ',score_lor)\n",
    "print(\"Decision Tree: \",score_dt)\n",
    "print(\"Random Forest: \",score_rt)\n",
    "print(\"knn: \",accuracy)\n",
    "print(\"AdaBoost: \",score_AB)\n",
    "print(\"XGBoost: \",score_XB)\n",
    "\n",
    "mse_list = cross_val_score(my_linreg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_list_positive = -mse_list\n",
    "rmse_list = np.sqrt(mse_list_positive)\n",
    "print(\"Linear Regression: \",rmse_list.mean())\n",
    "\n",
    "accuracy_list = cross_val_score(my_logreg, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"Logistic Regression: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_decisiontree, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"decisiontree: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_RandomForest, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"Random Forest: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"KNN: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_AdaBoost, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_ab = accuracy_list.mean()\n",
    "print(\"Adaboost : \",accuracy_ab)\n",
    "\n",
    "accuracy_list = cross_val_score(my_XGBoost, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_XG = accuracy_list.mean()\n",
    "print(\"XGBoost: \",accuracy_XG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70844122\n",
      "Iteration 2, loss = 0.69126202\n",
      "Iteration 3, loss = 0.68853412\n",
      "Iteration 4, loss = 0.69056783\n",
      "Iteration 5, loss = 0.69370731\n",
      "Iteration 6, loss = 0.69096780\n",
      "Iteration 7, loss = 0.70094691\n",
      "Iteration 8, loss = 0.70216289\n",
      "Iteration 9, loss = 0.69267004\n",
      "Iteration 10, loss = 0.69145461\n",
      "Iteration 11, loss = 0.69108823\n",
      "Iteration 12, loss = 0.69159680\n",
      "Iteration 13, loss = 0.69420241\n",
      "Iteration 14, loss = 0.69707153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70655780\n",
      "Iteration 2, loss = 0.69091895\n",
      "Iteration 3, loss = 0.69006396\n",
      "Iteration 4, loss = 0.69368305\n",
      "Iteration 5, loss = 0.69753246\n",
      "Iteration 6, loss = 0.69096927\n",
      "Iteration 7, loss = 0.70262084\n",
      "Iteration 8, loss = 0.70102910\n",
      "Iteration 9, loss = 0.69146328\n",
      "Iteration 10, loss = 0.69175728\n",
      "Iteration 11, loss = 0.69175006\n",
      "Iteration 12, loss = 0.69435144\n",
      "Iteration 13, loss = 0.69761925\n",
      "Iteration 14, loss = 0.69399005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70951273\n",
      "Iteration 2, loss = 0.68963761\n",
      "Iteration 3, loss = 0.68557315\n",
      "Iteration 4, loss = 0.68877362\n",
      "Iteration 5, loss = 0.69268091\n",
      "Iteration 6, loss = 0.68921486\n",
      "Iteration 7, loss = 0.69974319\n",
      "Iteration 8, loss = 0.69237577\n",
      "Iteration 9, loss = 0.68921883\n",
      "Iteration 10, loss = 0.69157908\n",
      "Iteration 11, loss = 0.69132528\n",
      "Iteration 12, loss = 0.69106535\n",
      "Iteration 13, loss = 0.69630966\n",
      "Iteration 14, loss = 0.68879234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70737068\n",
      "Iteration 2, loss = 0.69355298\n",
      "Iteration 3, loss = 0.68872388\n",
      "Iteration 4, loss = 0.69436481\n",
      "Iteration 5, loss = 0.69181572\n",
      "Iteration 6, loss = 0.69665364\n",
      "Iteration 7, loss = 0.70577824\n",
      "Iteration 8, loss = 0.69390937\n",
      "Iteration 9, loss = 0.68836748\n",
      "Iteration 10, loss = 0.69107298\n",
      "Iteration 11, loss = 0.69393111\n",
      "Iteration 12, loss = 0.69188818\n",
      "Iteration 13, loss = 0.69692041\n",
      "Iteration 14, loss = 0.69105064\n",
      "Iteration 15, loss = 0.69117451\n",
      "Iteration 16, loss = 0.69184028\n",
      "Iteration 17, loss = 0.69824555\n",
      "Iteration 18, loss = 0.69318716\n",
      "Iteration 19, loss = 0.69098511\n",
      "Iteration 20, loss = 0.69444855\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70837889\n",
      "Iteration 2, loss = 0.69457961\n",
      "Iteration 3, loss = 0.69393035\n",
      "Iteration 4, loss = 0.69395786\n",
      "Iteration 5, loss = 0.69215839\n",
      "Iteration 6, loss = 0.69385851\n",
      "Iteration 7, loss = 0.70107320\n",
      "Iteration 8, loss = 0.69512684\n",
      "Iteration 9, loss = 0.69216009\n",
      "Iteration 10, loss = 0.69366210\n",
      "Iteration 11, loss = 0.69642787\n",
      "Iteration 12, loss = 0.69399586\n",
      "Iteration 13, loss = 0.69510427\n",
      "Iteration 14, loss = 0.69299628\n",
      "Iteration 15, loss = 0.69296595\n",
      "Iteration 16, loss = 0.69329787\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70309988\n",
      "Iteration 2, loss = 0.69168246\n",
      "Iteration 3, loss = 0.68758263\n",
      "Iteration 4, loss = 0.69172640\n",
      "Iteration 5, loss = 0.68753731\n",
      "Iteration 6, loss = 0.68851112\n",
      "Iteration 7, loss = 0.69129697\n",
      "Iteration 8, loss = 0.68983831\n",
      "Iteration 9, loss = 0.68990152\n",
      "Iteration 10, loss = 0.69089471\n",
      "Iteration 11, loss = 0.69874957\n",
      "Iteration 12, loss = 0.71737537\n",
      "Iteration 13, loss = 0.70005379\n",
      "Iteration 14, loss = 0.69420247\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70142691\n",
      "Iteration 2, loss = 0.68996107\n",
      "Iteration 3, loss = 0.68536870\n",
      "Iteration 4, loss = 0.68966413\n",
      "Iteration 5, loss = 0.68452978\n",
      "Iteration 6, loss = 0.68537957\n",
      "Iteration 7, loss = 0.68819869\n",
      "Iteration 8, loss = 0.68609216\n",
      "Iteration 9, loss = 0.68632737\n",
      "Iteration 10, loss = 0.69001123\n",
      "Iteration 11, loss = 0.68791933\n",
      "Iteration 12, loss = 0.69794852\n",
      "Iteration 13, loss = 0.68679686\n",
      "Iteration 14, loss = 0.68590260\n",
      "Iteration 15, loss = 0.68739150\n",
      "Iteration 16, loss = 0.68558905\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70623306\n",
      "Iteration 2, loss = 0.69156683\n",
      "Iteration 3, loss = 0.69115343\n",
      "Iteration 4, loss = 0.69344585\n",
      "Iteration 5, loss = 0.69014581\n",
      "Iteration 6, loss = 0.68842939\n",
      "Iteration 7, loss = 0.69245127\n",
      "Iteration 8, loss = 0.68935025\n",
      "Iteration 9, loss = 0.68983027\n",
      "Iteration 10, loss = 0.69526840\n",
      "Iteration 11, loss = 0.69254107\n",
      "Iteration 12, loss = 0.70222546\n",
      "Iteration 13, loss = 0.69500957\n",
      "Iteration 14, loss = 0.69179087\n",
      "Iteration 15, loss = 0.69391977\n",
      "Iteration 16, loss = 0.68897611\n",
      "Iteration 17, loss = 0.68977074\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70405486\n",
      "Iteration 2, loss = 0.69181063\n",
      "Iteration 3, loss = 0.69088466\n",
      "Iteration 4, loss = 0.69400932\n",
      "Iteration 5, loss = 0.69066707\n",
      "Iteration 6, loss = 0.68988250\n",
      "Iteration 7, loss = 0.68939444\n",
      "Iteration 8, loss = 0.68921504\n",
      "Iteration 9, loss = 0.69005612\n",
      "Iteration 10, loss = 0.70229946\n",
      "Iteration 11, loss = 0.69422702\n",
      "Iteration 12, loss = 0.70477316\n",
      "Iteration 13, loss = 0.69290680\n",
      "Iteration 14, loss = 0.69355269\n",
      "Iteration 15, loss = 0.68975166\n",
      "Iteration 16, loss = 0.69064025\n",
      "Iteration 17, loss = 0.69108873\n",
      "Iteration 18, loss = 0.69240727\n",
      "Iteration 19, loss = 0.69140827\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70451797\n",
      "Iteration 2, loss = 0.69013473\n",
      "Iteration 3, loss = 0.68979071\n",
      "Iteration 4, loss = 0.69466413\n",
      "Iteration 5, loss = 0.68851434\n",
      "Iteration 6, loss = 0.68933508\n",
      "Iteration 7, loss = 0.68837424\n",
      "Iteration 8, loss = 0.68983365\n",
      "Iteration 9, loss = 0.69281075\n",
      "Iteration 10, loss = 0.70252743\n",
      "Iteration 11, loss = 0.69933793\n",
      "Iteration 12, loss = 0.70689892\n",
      "Iteration 13, loss = 0.69387508\n",
      "Iteration 14, loss = 0.69030358\n",
      "Iteration 15, loss = 0.69115399\n",
      "Iteration 16, loss = 0.68948714\n",
      "Iteration 17, loss = 0.68941514\n",
      "Iteration 18, loss = 0.69158847\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.5107384169884169\n",
      "Accuracy for ML is 51.07384169884169%\n"
     ]
    }
   ],
   "source": [
    "preprocessing_features = preprocessing.scale(X)\n",
    "preprocessing_features\n",
    "\n",
    "my_ANN = MLPClassifier(hidden_layer_sizes=(30,), activation= 'logistic', \n",
    "                       solver='adam', alpha=1, random_state=1, \n",
    "                       learning_rate_init = 0.02, verbose=True)\n",
    "\n",
    "accuracy_list = cross_val_score(my_ANN, preprocessing_features, y, cv=10, scoring='accuracy')\n",
    "\n",
    "accuracy_cv_mean = accuracy_list.mean()\n",
    "\n",
    "print(accuracy_cv_mean)\n",
    "print(\"Accuracy for ML is \" + str((accuracy_cv_mean * 100)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Linear Regression 0.49425579970215155\n",
      "LogisticRegression:  0.5770609318996416\n",
      "Decision Tree:  0.5698924731182796\n",
      "Random Forest:  0.5734767025089605\n",
      "knn:  0.4767025089605735\n",
      "AdaBoost:  0.578853046594982\n",
      "XGBoost:  0.5716845878136201\n",
      "Linear Regression:  0.49712753948225075\n",
      "Logistic Regression: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.4932553088803088\n",
      "decisiontree:  0.4309684684684685\n",
      "Random Forest:  0.4332006113256113\n",
      "KNN:  0.41886261261261265\n",
      "Adaboost :  0.46760376447876445\n",
      "[21:21:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21:21:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost:  0.4296171171171171\n"
     ]
    }
   ],
   "source": [
    "#feature_cols = ['Stock name','Volume avg','Volitity (1 or 0)','Higher than MVA50','Higher than MVA100','Higher than MVA100','Higher than EMVA 1']\n",
    "feature_cols_No_MVA100 = ['Stock name','Volume avg','Volitity (1 or 0)','Higher than MVA50','Higher than MVA100','Higher than EMVA 1']\n",
    "X = df[feature_cols_No_MVA100]\n",
    "y = df['higher closing 0 or 1']\n",
    "\n",
    "#seting up train set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "my_logreg.fit(X_train, y_train)\n",
    "my_RandomForest.fit(X_train, y_train)\n",
    "my_decisiontree.fit(X_train, y_train)\n",
    "my_linreg.fit(X_train, y_train)\n",
    "my_AdaBoost.fit(X_train,y_train)\n",
    "my_XGBoost.fit(X_train,y_train)\n",
    "\n",
    "#Training\n",
    "y_predict_lor = my_logreg.predict(X_test)\n",
    "y_predict = knn.predict(X_test)\n",
    "y_predict_dt = my_decisiontree.predict(X_test)\n",
    "y_predict_rt = my_RandomForest.predict(X_test)\n",
    "y_predict_LR = my_linreg.predict(X_test)\n",
    "y_predict_AB = my_AdaBoost.predict(X_test)\n",
    "y_predict_XB = my_XGBoost.predict(X_test)\n",
    "\n",
    "#accuracy of the perdiction\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_lor = accuracy_score(y_test, y_predict_lor)\n",
    "score_dt = accuracy_score(y_test, y_predict_dt)\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "score_rt = accuracy_score(y_test,y_predict_rt)\n",
    "score_AB = accuracy_score(y_test, y_predict_AB)\n",
    "score_XB = accuracy_score(y_test, y_predict_XB)\n",
    "\n",
    "y_predict_LR = my_linreg.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test, y_predict_LR)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Linear Regression\",rmse)\n",
    "print('LogisticRegression: ',score_lor)\n",
    "print(\"Decision Tree: \",score_dt)\n",
    "print(\"Random Forest: \",score_rt)\n",
    "print(\"knn: \",accuracy)\n",
    "print(\"AdaBoost: \",score_AB)\n",
    "print(\"XGBoost: \",score_XB)\n",
    "\n",
    "mse_list = cross_val_score(my_linreg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_list_positive = -mse_list\n",
    "rmse_list = np.sqrt(mse_list_positive)\n",
    "print(\"Linear Regression: \",rmse_list.mean())\n",
    "\n",
    "accuracy_list = cross_val_score(my_logreg, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"Logistic Regression: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_decisiontree, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"decisiontree: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_RandomForest, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"Random Forest: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"KNN: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_AdaBoost, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_ab = accuracy_list.mean()\n",
    "print(\"Adaboost : \",accuracy_ab)\n",
    "\n",
    "accuracy_list = cross_val_score(my_XGBoost, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_XG = accuracy_list.mean()\n",
    "print(\"XGBoost: \",accuracy_XG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69664251\n",
      "Iteration 2, loss = 0.69015354\n",
      "Iteration 3, loss = 0.69045916\n",
      "Iteration 4, loss = 0.70294732\n",
      "Iteration 5, loss = 0.69049225\n",
      "Iteration 6, loss = 0.69189355\n",
      "Iteration 7, loss = 0.68874607\n",
      "Iteration 8, loss = 0.69137206\n",
      "Iteration 9, loss = 0.68842802\n",
      "Iteration 10, loss = 0.69260204\n",
      "Iteration 11, loss = 0.69946004\n",
      "Iteration 12, loss = 0.68825308\n",
      "Iteration 13, loss = 0.68879131\n",
      "Iteration 14, loss = 0.69588369\n",
      "Iteration 15, loss = 0.69164209\n",
      "Iteration 16, loss = 0.69270426\n",
      "Iteration 17, loss = 0.69311327\n",
      "Iteration 18, loss = 0.69042326\n",
      "Iteration 19, loss = 0.69003976\n",
      "Iteration 20, loss = 0.69060657\n",
      "Iteration 21, loss = 0.68952447\n",
      "Iteration 22, loss = 0.69472355\n",
      "Iteration 23, loss = 0.69384379\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69746371\n",
      "Iteration 2, loss = 0.69255591\n",
      "Iteration 3, loss = 0.69345499\n",
      "Iteration 4, loss = 0.70385583\n",
      "Iteration 5, loss = 0.68927348\n",
      "Iteration 6, loss = 0.69162154\n",
      "Iteration 7, loss = 0.68934425\n",
      "Iteration 8, loss = 0.68988281\n",
      "Iteration 9, loss = 0.68975910\n",
      "Iteration 10, loss = 0.69859555\n",
      "Iteration 11, loss = 0.70012802\n",
      "Iteration 12, loss = 0.68970423\n",
      "Iteration 13, loss = 0.68958045\n",
      "Iteration 14, loss = 0.69795432\n",
      "Iteration 15, loss = 0.69197658\n",
      "Iteration 16, loss = 0.69709109\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69599016\n",
      "Iteration 2, loss = 0.68723419\n",
      "Iteration 3, loss = 0.68906749\n",
      "Iteration 4, loss = 0.71303176\n",
      "Iteration 5, loss = 0.68796905\n",
      "Iteration 6, loss = 0.68742101\n",
      "Iteration 7, loss = 0.68604293\n",
      "Iteration 8, loss = 0.68858022\n",
      "Iteration 9, loss = 0.68692436\n",
      "Iteration 10, loss = 0.68880714\n",
      "Iteration 11, loss = 0.68890277\n",
      "Iteration 12, loss = 0.68634406\n",
      "Iteration 13, loss = 0.68652374\n",
      "Iteration 14, loss = 0.69571891\n",
      "Iteration 15, loss = 0.68963159\n",
      "Iteration 16, loss = 0.69037334\n",
      "Iteration 17, loss = 0.69229666\n",
      "Iteration 18, loss = 0.68822177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69959678\n",
      "Iteration 2, loss = 0.68858111\n",
      "Iteration 3, loss = 0.69090438\n",
      "Iteration 4, loss = 0.70400143\n",
      "Iteration 5, loss = 0.68964703\n",
      "Iteration 6, loss = 0.69053057\n",
      "Iteration 7, loss = 0.68829657\n",
      "Iteration 8, loss = 0.68941666\n",
      "Iteration 9, loss = 0.68854812\n",
      "Iteration 10, loss = 0.69043378\n",
      "Iteration 11, loss = 0.68953103\n",
      "Iteration 12, loss = 0.69056628\n",
      "Iteration 13, loss = 0.69523738\n",
      "Iteration 14, loss = 0.68922931\n",
      "Iteration 15, loss = 0.69149087\n",
      "Iteration 16, loss = 0.69265269\n",
      "Iteration 17, loss = 0.69028524\n",
      "Iteration 18, loss = 0.68976419\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69992326\n",
      "Iteration 2, loss = 0.69060238\n",
      "Iteration 3, loss = 0.69828797\n",
      "Iteration 4, loss = 0.69977028\n",
      "Iteration 5, loss = 0.69280486\n",
      "Iteration 6, loss = 0.69682744\n",
      "Iteration 7, loss = 0.69678216\n",
      "Iteration 8, loss = 0.69234857\n",
      "Iteration 9, loss = 0.69103632\n",
      "Iteration 10, loss = 0.69240376\n",
      "Iteration 11, loss = 0.69156626\n",
      "Iteration 12, loss = 0.69290493\n",
      "Iteration 13, loss = 0.69160678\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69846167\n",
      "Iteration 2, loss = 0.70226471\n",
      "Iteration 3, loss = 0.69013500\n",
      "Iteration 4, loss = 0.68812110\n",
      "Iteration 5, loss = 0.68899623\n",
      "Iteration 6, loss = 0.68787745\n",
      "Iteration 7, loss = 0.68686837\n",
      "Iteration 8, loss = 0.69684378\n",
      "Iteration 9, loss = 0.68877369\n",
      "Iteration 10, loss = 0.69064789\n",
      "Iteration 11, loss = 0.69672713\n",
      "Iteration 12, loss = 0.69392429\n",
      "Iteration 13, loss = 0.68790598\n",
      "Iteration 14, loss = 0.68974353\n",
      "Iteration 15, loss = 0.69021444\n",
      "Iteration 16, loss = 0.69382924\n",
      "Iteration 17, loss = 0.68919710\n",
      "Iteration 18, loss = 0.69599169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69651972\n",
      "Iteration 2, loss = 0.69675022\n",
      "Iteration 3, loss = 0.68723778\n",
      "Iteration 4, loss = 0.68622473\n",
      "Iteration 5, loss = 0.68488821\n",
      "Iteration 6, loss = 0.68583100\n",
      "Iteration 7, loss = 0.68561833\n",
      "Iteration 8, loss = 0.68918011\n",
      "Iteration 9, loss = 0.68815065\n",
      "Iteration 10, loss = 0.69341827\n",
      "Iteration 11, loss = 0.68996024\n",
      "Iteration 12, loss = 0.69366600\n",
      "Iteration 13, loss = 0.68823216\n",
      "Iteration 14, loss = 0.69031173\n",
      "Iteration 15, loss = 0.68709220\n",
      "Iteration 16, loss = 0.69122007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70741973\n",
      "Iteration 2, loss = 0.69117062\n",
      "Iteration 3, loss = 0.68806707\n",
      "Iteration 4, loss = 0.69455630\n",
      "Iteration 5, loss = 0.68793154\n",
      "Iteration 6, loss = 0.69086875\n",
      "Iteration 7, loss = 0.68798672\n",
      "Iteration 8, loss = 0.69140684\n",
      "Iteration 9, loss = 0.68822245\n",
      "Iteration 10, loss = 0.69549828\n",
      "Iteration 11, loss = 0.69475832\n",
      "Iteration 12, loss = 0.69555114\n",
      "Iteration 13, loss = 0.69336139\n",
      "Iteration 14, loss = 0.69690814\n",
      "Iteration 15, loss = 0.69464277\n",
      "Iteration 16, loss = 0.68859419\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70215473\n",
      "Iteration 2, loss = 0.69480126\n",
      "Iteration 3, loss = 0.69414591\n",
      "Iteration 4, loss = 0.70063989\n",
      "Iteration 5, loss = 0.68940275\n",
      "Iteration 6, loss = 0.69298960\n",
      "Iteration 7, loss = 0.68982070\n",
      "Iteration 8, loss = 0.68930434\n",
      "Iteration 9, loss = 0.68985955\n",
      "Iteration 10, loss = 0.69735949\n",
      "Iteration 11, loss = 0.69668701\n",
      "Iteration 12, loss = 0.70080414\n",
      "Iteration 13, loss = 0.69657208\n",
      "Iteration 14, loss = 0.70036356\n",
      "Iteration 15, loss = 0.70423812\n",
      "Iteration 16, loss = 0.69234744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69768055\n",
      "Iteration 2, loss = 0.69683726\n",
      "Iteration 3, loss = 0.69056092\n",
      "Iteration 4, loss = 0.69298226\n",
      "Iteration 5, loss = 0.69292937\n",
      "Iteration 6, loss = 0.68999724\n",
      "Iteration 7, loss = 0.69063171\n",
      "Iteration 8, loss = 0.68966109\n",
      "Iteration 9, loss = 0.68946920\n",
      "Iteration 10, loss = 0.70297482\n",
      "Iteration 11, loss = 0.69517467\n",
      "Iteration 12, loss = 0.69584016\n",
      "Iteration 13, loss = 0.69161687\n",
      "Iteration 14, loss = 0.69625179\n",
      "Iteration 15, loss = 0.70111983\n",
      "Iteration 16, loss = 0.69235976\n",
      "Iteration 17, loss = 0.69480859\n",
      "Iteration 18, loss = 0.68992312\n",
      "Iteration 19, loss = 0.68912446\n",
      "Iteration 20, loss = 0.68984008\n",
      "Iteration 21, loss = 0.69016014\n",
      "Iteration 22, loss = 0.69078140\n",
      "Iteration 23, loss = 0.69036808\n",
      "Iteration 24, loss = 0.68995841\n",
      "Iteration 25, loss = 0.69292384\n",
      "Iteration 26, loss = 0.69044501\n",
      "Iteration 27, loss = 0.68908066\n",
      "Iteration 28, loss = 0.69204091\n",
      "Iteration 29, loss = 0.69805093\n",
      "Iteration 30, loss = 0.70313527\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.49673825611325617\n",
      "Accuracy for ML is 49.67382561132562%\n"
     ]
    }
   ],
   "source": [
    "preprocessing_features = preprocessing.scale(X)\n",
    "preprocessing_features\n",
    "\n",
    "my_ANN = MLPClassifier(hidden_layer_sizes=(30,), activation= 'logistic', \n",
    "                       solver='adam', alpha=1, random_state=1, \n",
    "                       learning_rate_init = 0.02, verbose=True)\n",
    "\n",
    "accuracy_list = cross_val_score(my_ANN, preprocessing_features, y, cv=10, scoring='accuracy')\n",
    "\n",
    "accuracy_cv_mean = accuracy_list.mean()\n",
    "\n",
    "print(accuracy_cv_mean)\n",
    "print(\"Accuracy for ML is \" + str((accuracy_cv_mean * 100)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:11:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Linear Regression 0.495515260382338\n",
      "LogisticRegression:  0.5734767025089605\n",
      "Decision Tree:  0.5501792114695341\n",
      "Random Forest:  0.546594982078853\n",
      "knn:  0.4838709677419355\n",
      "AdaBoost:  0.5645161290322581\n",
      "XGBoost:  0.546594982078853\n",
      "Linear Regression:  0.49736537225974675\n",
      "Logistic Regression:  0.5045286357786358\n",
      "decisiontree: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.45962837837837844\n",
      "Random Forest:  0.43059443371943373\n",
      "KNN:  0.46379504504504504\n",
      "Adaboost :  0.47753378378378375\n",
      "[21:11:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:11:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:11:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost:  0.4591658622908623\n"
     ]
    }
   ],
   "source": [
    "#feature_cols = ['Stock name','Volume avg','Volitity (1 or 0)','Higher than MVA50','Higher than MVA100','feature_cols_No_EMVA']\n",
    "feature_cols_No_MVA50 = ['Stock name','Volume avg','Volitity (1 or 0)','Higher than MVA100','Higher than EMVA 1']\n",
    "X = df[feature_cols_No_MVA50]\n",
    "y = df['higher closing 0 or 1']\n",
    "\n",
    "#seting up train set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "my_logreg.fit(X_train, y_train)\n",
    "my_RandomForest.fit(X_train, y_train)\n",
    "my_decisiontree.fit(X_train, y_train)\n",
    "my_linreg.fit(X_train, y_train)\n",
    "my_AdaBoost.fit(X_train,y_train)\n",
    "my_XGBoost.fit(X_train,y_train)\n",
    "\n",
    "#Training\n",
    "y_predict_lor = my_logreg.predict(X_test)\n",
    "y_predict = knn.predict(X_test)\n",
    "y_predict_dt = my_decisiontree.predict(X_test)\n",
    "y_predict_rt = my_RandomForest.predict(X_test)\n",
    "y_predict_LR = my_linreg.predict(X_test)\n",
    "y_predict_AB = my_AdaBoost.predict(X_test)\n",
    "y_predict_XB = my_XGBoost.predict(X_test)\n",
    "\n",
    "#accuracy of the perdiction\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_lor = accuracy_score(y_test, y_predict_lor)\n",
    "score_dt = accuracy_score(y_test, y_predict_dt)\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "score_rt = accuracy_score(y_test,y_predict_rt)\n",
    "score_AB = accuracy_score(y_test, y_predict_AB)\n",
    "score_XB = accuracy_score(y_test, y_predict_XB)\n",
    "\n",
    "y_predict_LR = my_linreg.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test, y_predict_LR)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Linear Regression\",rmse)\n",
    "print('LogisticRegression: ',score_lor)\n",
    "print(\"Decision Tree: \",score_dt)\n",
    "print(\"Random Forest: \",score_rt)\n",
    "print(\"knn: \",accuracy)\n",
    "print(\"AdaBoost: \",score_AB)\n",
    "print(\"XGBoost: \",score_XB)\n",
    "\n",
    "mse_list = cross_val_score(my_linreg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_list_positive = -mse_list\n",
    "rmse_list = np.sqrt(mse_list_positive)\n",
    "print(\"Linear Regression: \",rmse_list.mean())\n",
    "\n",
    "accuracy_list = cross_val_score(my_logreg, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"Logistic Regression: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_decisiontree, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"decisiontree: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_RandomForest, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"Random Forest: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"KNN: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_AdaBoost, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_ab = accuracy_list.mean()\n",
    "print(\"Adaboost : \",accuracy_ab)\n",
    "\n",
    "accuracy_list = cross_val_score(my_XGBoost, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_XG = accuracy_list.mean()\n",
    "print(\"XGBoost: \",accuracy_XG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70734459\n",
      "Iteration 2, loss = 0.69174705\n",
      "Iteration 3, loss = 0.68949695\n",
      "Iteration 4, loss = 0.69085201\n",
      "Iteration 5, loss = 0.69406345\n",
      "Iteration 6, loss = 0.69168366\n",
      "Iteration 7, loss = 0.70248889\n",
      "Iteration 8, loss = 0.70328733\n",
      "Iteration 9, loss = 0.69330718\n",
      "Iteration 10, loss = 0.69208533\n",
      "Iteration 11, loss = 0.69150902\n",
      "Iteration 12, loss = 0.69190205\n",
      "Iteration 13, loss = 0.69486037\n",
      "Iteration 14, loss = 0.69779720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70561503\n",
      "Iteration 2, loss = 0.69158441\n",
      "Iteration 3, loss = 0.69125918\n",
      "Iteration 4, loss = 0.69428773\n",
      "Iteration 5, loss = 0.69789774\n",
      "Iteration 6, loss = 0.69176699\n",
      "Iteration 7, loss = 0.70422975\n",
      "Iteration 8, loss = 0.70218986\n",
      "Iteration 9, loss = 0.69211033\n",
      "Iteration 10, loss = 0.69226917\n",
      "Iteration 11, loss = 0.69209984\n",
      "Iteration 12, loss = 0.69464495\n",
      "Iteration 13, loss = 0.69803482\n",
      "Iteration 14, loss = 0.69442842\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70828797\n",
      "Iteration 2, loss = 0.69047219\n",
      "Iteration 3, loss = 0.68666980\n",
      "Iteration 4, loss = 0.68928259\n",
      "Iteration 5, loss = 0.69330571\n",
      "Iteration 6, loss = 0.69030964\n",
      "Iteration 7, loss = 0.70169353\n",
      "Iteration 8, loss = 0.69370489\n",
      "Iteration 9, loss = 0.69024834\n",
      "Iteration 10, loss = 0.69234434\n",
      "Iteration 11, loss = 0.69198109\n",
      "Iteration 12, loss = 0.69159057\n",
      "Iteration 13, loss = 0.69720345\n",
      "Iteration 14, loss = 0.68975721\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70626567\n",
      "Iteration 2, loss = 0.69448228\n",
      "Iteration 3, loss = 0.68927708\n",
      "Iteration 4, loss = 0.69467102\n",
      "Iteration 5, loss = 0.69245776\n",
      "Iteration 6, loss = 0.69799946\n",
      "Iteration 7, loss = 0.70766802\n",
      "Iteration 8, loss = 0.69503322\n",
      "Iteration 9, loss = 0.68915569\n",
      "Iteration 10, loss = 0.69173766\n",
      "Iteration 11, loss = 0.69437071\n",
      "Iteration 12, loss = 0.69227006\n",
      "Iteration 13, loss = 0.69763383\n",
      "Iteration 14, loss = 0.69174865\n",
      "Iteration 15, loss = 0.69189693\n",
      "Iteration 16, loss = 0.69263471\n",
      "Iteration 17, loss = 0.69869082\n",
      "Iteration 18, loss = 0.69358873\n",
      "Iteration 19, loss = 0.69150909\n",
      "Iteration 20, loss = 0.69476149\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70714885\n",
      "Iteration 2, loss = 0.69484190\n",
      "Iteration 3, loss = 0.69379543\n",
      "Iteration 4, loss = 0.69380250\n",
      "Iteration 5, loss = 0.69222285\n",
      "Iteration 6, loss = 0.69430464\n",
      "Iteration 7, loss = 0.70173401\n",
      "Iteration 8, loss = 0.69546068\n",
      "Iteration 9, loss = 0.69227773\n",
      "Iteration 10, loss = 0.69376649\n",
      "Iteration 11, loss = 0.69650392\n",
      "Iteration 12, loss = 0.69373453\n",
      "Iteration 13, loss = 0.69497197\n",
      "Iteration 14, loss = 0.69298683\n",
      "Iteration 15, loss = 0.69305983\n",
      "Iteration 16, loss = 0.69337750\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70143269\n",
      "Iteration 2, loss = 0.69294606\n",
      "Iteration 3, loss = 0.68922479\n",
      "Iteration 4, loss = 0.69355575\n",
      "Iteration 5, loss = 0.68894254\n",
      "Iteration 6, loss = 0.68929158\n",
      "Iteration 7, loss = 0.69220360\n",
      "Iteration 8, loss = 0.69069335\n",
      "Iteration 9, loss = 0.69158378\n",
      "Iteration 10, loss = 0.69242938\n",
      "Iteration 11, loss = 0.70003352\n",
      "Iteration 12, loss = 0.71882372\n",
      "Iteration 13, loss = 0.70106480\n",
      "Iteration 14, loss = 0.69546365\n",
      "Iteration 15, loss = 0.69056212\n",
      "Iteration 16, loss = 0.69124460\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70011258\n",
      "Iteration 2, loss = 0.69199250\n",
      "Iteration 3, loss = 0.68735298\n",
      "Iteration 4, loss = 0.69222357\n",
      "Iteration 5, loss = 0.68664898\n",
      "Iteration 6, loss = 0.68725781\n",
      "Iteration 7, loss = 0.69032023\n",
      "Iteration 8, loss = 0.68820695\n",
      "Iteration 9, loss = 0.68843116\n",
      "Iteration 10, loss = 0.69219721\n",
      "Iteration 11, loss = 0.68987206\n",
      "Iteration 12, loss = 0.69999681\n",
      "Iteration 13, loss = 0.68887051\n",
      "Iteration 14, loss = 0.68823354\n",
      "Iteration 15, loss = 0.68997249\n",
      "Iteration 16, loss = 0.68832017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70383814\n",
      "Iteration 2, loss = 0.69065805\n",
      "Iteration 3, loss = 0.69040788\n",
      "Iteration 4, loss = 0.69310952\n",
      "Iteration 5, loss = 0.68931201\n",
      "Iteration 6, loss = 0.68772711\n",
      "Iteration 7, loss = 0.69170597\n",
      "Iteration 8, loss = 0.68873222\n",
      "Iteration 9, loss = 0.68916297\n",
      "Iteration 10, loss = 0.69478115\n",
      "Iteration 11, loss = 0.69191126\n",
      "Iteration 12, loss = 0.70132485\n",
      "Iteration 13, loss = 0.69367282\n",
      "Iteration 14, loss = 0.69062150\n",
      "Iteration 15, loss = 0.69293698\n",
      "Iteration 16, loss = 0.68820170\n",
      "Iteration 17, loss = 0.68910861\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70486803\n",
      "Iteration 2, loss = 0.69399592\n",
      "Iteration 3, loss = 0.69308401\n",
      "Iteration 4, loss = 0.69630168\n",
      "Iteration 5, loss = 0.69249336\n",
      "Iteration 6, loss = 0.69182878\n",
      "Iteration 7, loss = 0.69140223\n",
      "Iteration 8, loss = 0.69146663\n",
      "Iteration 9, loss = 0.69210056\n",
      "Iteration 10, loss = 0.70497575\n",
      "Iteration 11, loss = 0.69670416\n",
      "Iteration 12, loss = 0.70718646\n",
      "Iteration 13, loss = 0.69451729\n",
      "Iteration 14, loss = 0.69531984\n",
      "Iteration 15, loss = 0.69159535\n",
      "Iteration 16, loss = 0.69323586\n",
      "Iteration 17, loss = 0.69335096\n",
      "Iteration 18, loss = 0.69457794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70395201\n",
      "Iteration 2, loss = 0.69091285\n",
      "Iteration 3, loss = 0.69089053\n",
      "Iteration 4, loss = 0.69594417\n",
      "Iteration 5, loss = 0.68954125\n",
      "Iteration 6, loss = 0.69053241\n",
      "Iteration 7, loss = 0.68959006\n",
      "Iteration 8, loss = 0.69106265\n",
      "Iteration 9, loss = 0.69394739\n",
      "Iteration 10, loss = 0.70444630\n",
      "Iteration 11, loss = 0.70124364\n",
      "Iteration 12, loss = 0.70856413\n",
      "Iteration 13, loss = 0.69448833\n",
      "Iteration 14, loss = 0.69076393\n",
      "Iteration 15, loss = 0.69218049\n",
      "Iteration 16, loss = 0.69092956\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.5084982303732304\n",
      "Accuracy for ML is 50.84982303732304%\n"
     ]
    }
   ],
   "source": [
    "preprocessing_features = preprocessing.scale(X)\n",
    "preprocessing_features\n",
    "\n",
    "my_ANN = MLPClassifier(hidden_layer_sizes=(30,), activation= 'logistic', \n",
    "                       solver='adam', alpha=1, random_state=1, \n",
    "                       learning_rate_init = 0.02, verbose=True)\n",
    "\n",
    "accuracy_list = cross_val_score(my_ANN, preprocessing_features, y, cv=10, scoring='accuracy')\n",
    "\n",
    "accuracy_cv_mean = accuracy_list.mean()\n",
    "\n",
    "print(accuracy_cv_mean)\n",
    "print(\"Accuracy for ML is \" + str((accuracy_cv_mean * 100)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:14:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Linear Regression 0.49344653856856296\n",
      "LogisticRegression:  0.5716845878136201\n",
      "Decision Tree:  0.5698924731182796\n",
      "Random Forest:  0.5627240143369175\n",
      "knn:  0.4767025089605735\n",
      "AdaBoost:  0.578853046594982\n",
      "XGBoost:  0.5716845878136201\n",
      "Linear Regression:  0.4968061383607988\n",
      "Logistic Regression: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.5289736164736165\n",
      "decisiontree:  0.4309684684684685\n",
      "Random Forest:  0.4287121943371943\n",
      "KNN:  0.41886261261261265\n",
      "Adaboost :  0.46760376447876445\n",
      "[21:14:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21:14:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost:  0.4296171171171171\n"
     ]
    }
   ],
   "source": [
    "#feature_cols = ['Stock name','Volume avg','Volitity (1 or 0)','Higher than MVA50','Higher than MVA100','Higher than EMVA 1',]\n",
    "feature_cols_No_Volitility = ['Stock name','Volume avg','Higher than MVA50','Higher than MVA100','Higher than EMVA 1',]\n",
    "X = df[feature_cols_No_Volitility]\n",
    "y = df['higher closing 0 or 1']\n",
    "\n",
    "#seting up train set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "my_logreg.fit(X_train, y_train)\n",
    "my_RandomForest.fit(X_train, y_train)\n",
    "my_decisiontree.fit(X_train, y_train)\n",
    "my_linreg.fit(X_train, y_train)\n",
    "my_AdaBoost.fit(X_train,y_train)\n",
    "my_XGBoost.fit(X_train,y_train)\n",
    "\n",
    "#Training\n",
    "y_predict_lor = my_logreg.predict(X_test)\n",
    "y_predict = knn.predict(X_test)\n",
    "y_predict_dt = my_decisiontree.predict(X_test)\n",
    "y_predict_rt = my_RandomForest.predict(X_test)\n",
    "y_predict_LR = my_linreg.predict(X_test)\n",
    "y_predict_AB = my_AdaBoost.predict(X_test)\n",
    "y_predict_XB = my_XGBoost.predict(X_test)\n",
    "\n",
    "#accuracy of the perdiction\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_lor = accuracy_score(y_test, y_predict_lor)\n",
    "score_dt = accuracy_score(y_test, y_predict_dt)\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "score_rt = accuracy_score(y_test,y_predict_rt)\n",
    "score_AB = accuracy_score(y_test, y_predict_AB)\n",
    "score_XB = accuracy_score(y_test, y_predict_XB)\n",
    "\n",
    "y_predict_LR = my_linreg.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test, y_predict_LR)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Linear Regression\",rmse)\n",
    "print('LogisticRegression: ',score_lor)\n",
    "print(\"Decision Tree: \",score_dt)\n",
    "print(\"Random Forest: \",score_rt)\n",
    "print(\"knn: \",accuracy)\n",
    "print(\"AdaBoost: \",score_AB)\n",
    "print(\"XGBoost: \",score_XB)\n",
    "\n",
    "mse_list = cross_val_score(my_linreg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_list_positive = -mse_list\n",
    "rmse_list = np.sqrt(mse_list_positive)\n",
    "print(\"Linear Regression: \",rmse_list.mean())\n",
    "\n",
    "accuracy_list = cross_val_score(my_logreg, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"Logistic Regression: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_decisiontree, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"decisiontree: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_RandomForest, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"Random Forest: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"KNN: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_AdaBoost, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_ab = accuracy_list.mean()\n",
    "print(\"Adaboost : \",accuracy_ab)\n",
    "\n",
    "accuracy_list = cross_val_score(my_XGBoost, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_XG = accuracy_list.mean()\n",
    "print(\"XGBoost: \",accuracy_XG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70589535\n",
      "Iteration 2, loss = 0.68999007\n",
      "Iteration 3, loss = 0.68776894\n",
      "Iteration 4, loss = 0.68955887\n",
      "Iteration 5, loss = 0.69278130\n",
      "Iteration 6, loss = 0.69005953\n",
      "Iteration 7, loss = 0.70056038\n",
      "Iteration 8, loss = 0.70138239\n",
      "Iteration 9, loss = 0.69125109\n",
      "Iteration 10, loss = 0.69013002\n",
      "Iteration 11, loss = 0.68993197\n",
      "Iteration 12, loss = 0.69086588\n",
      "Iteration 13, loss = 0.69337594\n",
      "Iteration 14, loss = 0.69594484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70441449\n",
      "Iteration 2, loss = 0.68989315\n",
      "Iteration 3, loss = 0.69016325\n",
      "Iteration 4, loss = 0.69336816\n",
      "Iteration 5, loss = 0.69684750\n",
      "Iteration 6, loss = 0.69034982\n",
      "Iteration 7, loss = 0.70258256\n",
      "Iteration 8, loss = 0.70062473\n",
      "Iteration 9, loss = 0.69045036\n",
      "Iteration 10, loss = 0.69093190\n",
      "Iteration 11, loss = 0.69105766\n",
      "Iteration 12, loss = 0.69398401\n",
      "Iteration 13, loss = 0.69708442\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70693272\n",
      "Iteration 2, loss = 0.68932631\n",
      "Iteration 3, loss = 0.68557572\n",
      "Iteration 4, loss = 0.68857387\n",
      "Iteration 5, loss = 0.69249895\n",
      "Iteration 6, loss = 0.68872732\n",
      "Iteration 7, loss = 0.69932054\n",
      "Iteration 8, loss = 0.69167055\n",
      "Iteration 9, loss = 0.68810064\n",
      "Iteration 10, loss = 0.69060897\n",
      "Iteration 11, loss = 0.69088479\n",
      "Iteration 12, loss = 0.69092016\n",
      "Iteration 13, loss = 0.69586083\n",
      "Iteration 14, loss = 0.68792976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70501198\n",
      "Iteration 2, loss = 0.69282062\n",
      "Iteration 3, loss = 0.68783919\n",
      "Iteration 4, loss = 0.69305699\n",
      "Iteration 5, loss = 0.69062085\n",
      "Iteration 6, loss = 0.69632547\n",
      "Iteration 7, loss = 0.70585576\n",
      "Iteration 8, loss = 0.69340438\n",
      "Iteration 9, loss = 0.68716348\n",
      "Iteration 10, loss = 0.69020136\n",
      "Iteration 11, loss = 0.69284040\n",
      "Iteration 12, loss = 0.69118878\n",
      "Iteration 13, loss = 0.69623917\n",
      "Iteration 14, loss = 0.69012068\n",
      "Iteration 15, loss = 0.69026110\n",
      "Iteration 16, loss = 0.69089798\n",
      "Iteration 17, loss = 0.69675893\n",
      "Iteration 18, loss = 0.69151795\n",
      "Iteration 19, loss = 0.68963023\n",
      "Iteration 20, loss = 0.69324312\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70707100\n",
      "Iteration 2, loss = 0.69401430\n",
      "Iteration 3, loss = 0.69344318\n",
      "Iteration 4, loss = 0.69290957\n",
      "Iteration 5, loss = 0.69137004\n",
      "Iteration 6, loss = 0.69341726\n",
      "Iteration 7, loss = 0.70099608\n",
      "Iteration 8, loss = 0.69473120\n",
      "Iteration 9, loss = 0.69123386\n",
      "Iteration 10, loss = 0.69299901\n",
      "Iteration 11, loss = 0.69572353\n",
      "Iteration 12, loss = 0.69344253\n",
      "Iteration 13, loss = 0.69455116\n",
      "Iteration 14, loss = 0.69244107\n",
      "Iteration 15, loss = 0.69250604\n",
      "Iteration 16, loss = 0.69290477\n",
      "Iteration 17, loss = 0.69964062\n",
      "Iteration 18, loss = 0.69609099\n",
      "Iteration 19, loss = 0.69576630\n",
      "Iteration 20, loss = 0.70724358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70007570\n",
      "Iteration 2, loss = 0.69184382\n",
      "Iteration 3, loss = 0.68767885\n",
      "Iteration 4, loss = 0.69148330\n",
      "Iteration 5, loss = 0.68690667\n",
      "Iteration 6, loss = 0.68723761\n",
      "Iteration 7, loss = 0.69065954\n",
      "Iteration 8, loss = 0.68919780\n",
      "Iteration 9, loss = 0.68894564\n",
      "Iteration 10, loss = 0.69011330\n",
      "Iteration 11, loss = 0.69776690\n",
      "Iteration 12, loss = 0.71635197\n",
      "Iteration 13, loss = 0.69916868\n",
      "Iteration 14, loss = 0.69344511\n",
      "Iteration 15, loss = 0.68811123\n",
      "Iteration 16, loss = 0.68841324\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69782117\n",
      "Iteration 2, loss = 0.69031383\n",
      "Iteration 3, loss = 0.68553893\n",
      "Iteration 4, loss = 0.68925387\n",
      "Iteration 5, loss = 0.68390542\n",
      "Iteration 6, loss = 0.68435199\n",
      "Iteration 7, loss = 0.68732507\n",
      "Iteration 8, loss = 0.68569104\n",
      "Iteration 9, loss = 0.68523598\n",
      "Iteration 10, loss = 0.68952210\n",
      "Iteration 11, loss = 0.68663232\n",
      "Iteration 12, loss = 0.69644868\n",
      "Iteration 13, loss = 0.68577345\n",
      "Iteration 14, loss = 0.68542076\n",
      "Iteration 15, loss = 0.68620932\n",
      "Iteration 16, loss = 0.68476732\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70321204\n",
      "Iteration 2, loss = 0.69089855\n",
      "Iteration 3, loss = 0.69019788\n",
      "Iteration 4, loss = 0.69220735\n",
      "Iteration 5, loss = 0.68902120\n",
      "Iteration 6, loss = 0.68740667\n",
      "Iteration 7, loss = 0.69144776\n",
      "Iteration 8, loss = 0.68836048\n",
      "Iteration 9, loss = 0.68818074\n",
      "Iteration 10, loss = 0.69445323\n",
      "Iteration 11, loss = 0.69149755\n",
      "Iteration 12, loss = 0.70058676\n",
      "Iteration 13, loss = 0.69340670\n",
      "Iteration 14, loss = 0.69030083\n",
      "Iteration 15, loss = 0.69252475\n",
      "Iteration 16, loss = 0.68770078\n",
      "Iteration 17, loss = 0.68864517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70259881\n",
      "Iteration 2, loss = 0.69149090\n",
      "Iteration 3, loss = 0.69100512\n",
      "Iteration 4, loss = 0.69415229\n",
      "Iteration 5, loss = 0.69009236\n",
      "Iteration 6, loss = 0.68911058\n",
      "Iteration 7, loss = 0.68880789\n",
      "Iteration 8, loss = 0.68868450\n",
      "Iteration 9, loss = 0.68901239\n",
      "Iteration 10, loss = 0.70178751\n",
      "Iteration 11, loss = 0.69327093\n",
      "Iteration 12, loss = 0.70344101\n",
      "Iteration 13, loss = 0.69172218\n",
      "Iteration 14, loss = 0.69241144\n",
      "Iteration 15, loss = 0.68867721\n",
      "Iteration 16, loss = 0.68993345\n",
      "Iteration 17, loss = 0.69052216\n",
      "Iteration 18, loss = 0.69211921\n",
      "Iteration 19, loss = 0.69089020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70258165\n",
      "Iteration 2, loss = 0.68948835\n",
      "Iteration 3, loss = 0.68982195\n",
      "Iteration 4, loss = 0.69469218\n",
      "Iteration 5, loss = 0.68784269\n",
      "Iteration 6, loss = 0.68870341\n",
      "Iteration 7, loss = 0.68783765\n",
      "Iteration 8, loss = 0.68932141\n",
      "Iteration 9, loss = 0.69181377\n",
      "Iteration 10, loss = 0.70224768\n",
      "Iteration 11, loss = 0.69858900\n",
      "Iteration 12, loss = 0.70595192\n",
      "Iteration 13, loss = 0.69275245\n",
      "Iteration 14, loss = 0.68915101\n",
      "Iteration 15, loss = 0.69019180\n",
      "Iteration 16, loss = 0.68878635\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.5103161196911196\n",
      "Accuracy for ML is 51.03161196911196%\n"
     ]
    }
   ],
   "source": [
    "preprocessing_features = preprocessing.scale(X)\n",
    "preprocessing_features\n",
    "\n",
    "my_ANN = MLPClassifier(hidden_layer_sizes=(30,), activation= 'logistic', \n",
    "                       solver='adam', alpha=1, random_state=1, \n",
    "                       learning_rate_init = 0.02, verbose=True)\n",
    "\n",
    "accuracy_list = cross_val_score(my_ANN, preprocessing_features, y, cv=10, scoring='accuracy')\n",
    "\n",
    "accuracy_cv_mean = accuracy_list.mean()\n",
    "\n",
    "print(accuracy_cv_mean)\n",
    "print(\"Accuracy for ML is \" + str((accuracy_cv_mean * 100)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:16:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Linear Regression 0.49438822694546736\n",
      "LogisticRegression:  0.5752688172043011\n",
      "Decision Tree:  0.5734767025089605\n",
      "Random Forest:  0.5716845878136201\n",
      "knn:  0.48028673835125446\n",
      "AdaBoost:  0.5770609318996416\n",
      "XGBoost:  0.5752688172043011\n",
      "Linear Regression:  0.49694525989499966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.5186936936936937\n",
      "decisiontree:  0.4958252895752896\n",
      "Random Forest:  0.46190074002574005\n",
      "KNN:  0.49857625482625495\n",
      "Adaboost :  0.49124839124839126\n",
      "[21:16:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21:16:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:16:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost:  0.500365990990991\n"
     ]
    }
   ],
   "source": [
    "#feature_cols = ['Stock name','Volume avg','Volitity (1 or 0)','Higher than MVA50','Higher than MVA100','Higher than EMVA 1',]\n",
    "feature_cols_No_Volume = ['Stock name','Volitity (1 or 0)','Higher than MVA50','Higher than MVA100','Higher than EMVA 1',]\n",
    "X = df[feature_cols_No_Volume]\n",
    "y = df['higher closing 0 or 1']\n",
    "\n",
    "#seting up train set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "my_logreg.fit(X_train, y_train)\n",
    "my_RandomForest.fit(X_train, y_train)\n",
    "my_decisiontree.fit(X_train, y_train)\n",
    "my_linreg.fit(X_train, y_train)\n",
    "my_AdaBoost.fit(X_train,y_train)\n",
    "my_XGBoost.fit(X_train,y_train)\n",
    "\n",
    "#Training\n",
    "y_predict_lor = my_logreg.predict(X_test)\n",
    "y_predict = knn.predict(X_test)\n",
    "y_predict_dt = my_decisiontree.predict(X_test)\n",
    "y_predict_rt = my_RandomForest.predict(X_test)\n",
    "y_predict_LR = my_linreg.predict(X_test)\n",
    "y_predict_AB = my_AdaBoost.predict(X_test)\n",
    "y_predict_XB = my_XGBoost.predict(X_test)\n",
    "\n",
    "#accuracy of the perdiction\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_lor = accuracy_score(y_test, y_predict_lor)\n",
    "score_dt = accuracy_score(y_test, y_predict_dt)\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "score_rt = accuracy_score(y_test,y_predict_rt)\n",
    "score_AB = accuracy_score(y_test, y_predict_AB)\n",
    "score_XB = accuracy_score(y_test, y_predict_XB)\n",
    "\n",
    "y_predict_LR = my_linreg.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test, y_predict_LR)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Linear Regression\",rmse)\n",
    "print('LogisticRegression: ',score_lor)\n",
    "print(\"Decision Tree: \",score_dt)\n",
    "print(\"Random Forest: \",score_rt)\n",
    "print(\"knn: \",accuracy)\n",
    "print(\"AdaBoost: \",score_AB)\n",
    "print(\"XGBoost: \",score_XB)\n",
    "\n",
    "mse_list = cross_val_score(my_linreg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_list_positive = -mse_list\n",
    "rmse_list = np.sqrt(mse_list_positive)\n",
    "print(\"Linear Regression: \",rmse_list.mean())\n",
    "\n",
    "accuracy_list = cross_val_score(my_logreg, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"Logistic Regression: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_decisiontree, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"decisiontree: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_RandomForest, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"Random Forest: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "print(\"KNN: \",accuracy_cv)\n",
    "\n",
    "accuracy_list = cross_val_score(my_AdaBoost, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_ab = accuracy_list.mean()\n",
    "print(\"Adaboost : \",accuracy_ab)\n",
    "\n",
    "accuracy_list = cross_val_score(my_XGBoost, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_XG = accuracy_list.mean()\n",
    "print(\"XGBoost: \",accuracy_XG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70521725\n",
      "Iteration 2, loss = 0.69062425\n",
      "Iteration 3, loss = 0.68849297\n",
      "Iteration 4, loss = 0.69028759\n",
      "Iteration 5, loss = 0.69294621\n",
      "Iteration 6, loss = 0.69019900\n",
      "Iteration 7, loss = 0.70101994\n",
      "Iteration 8, loss = 0.70173958\n",
      "Iteration 9, loss = 0.69161184\n",
      "Iteration 10, loss = 0.69048383\n",
      "Iteration 11, loss = 0.69029952\n",
      "Iteration 12, loss = 0.69128596\n",
      "Iteration 13, loss = 0.69370392\n",
      "Iteration 14, loss = 0.69638727\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70392277\n",
      "Iteration 2, loss = 0.68980375\n",
      "Iteration 3, loss = 0.69026568\n",
      "Iteration 4, loss = 0.69367306\n",
      "Iteration 5, loss = 0.69678301\n",
      "Iteration 6, loss = 0.69046693\n",
      "Iteration 7, loss = 0.70264889\n",
      "Iteration 8, loss = 0.70055721\n",
      "Iteration 9, loss = 0.69049988\n",
      "Iteration 10, loss = 0.69102599\n",
      "Iteration 11, loss = 0.69113343\n",
      "Iteration 12, loss = 0.69412753\n",
      "Iteration 13, loss = 0.69715670\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70718316\n",
      "Iteration 2, loss = 0.68918917\n",
      "Iteration 3, loss = 0.68509928\n",
      "Iteration 4, loss = 0.68840846\n",
      "Iteration 5, loss = 0.69165623\n",
      "Iteration 6, loss = 0.68807533\n",
      "Iteration 7, loss = 0.69937137\n",
      "Iteration 8, loss = 0.69151948\n",
      "Iteration 9, loss = 0.68783600\n",
      "Iteration 10, loss = 0.69064293\n",
      "Iteration 11, loss = 0.69076030\n",
      "Iteration 12, loss = 0.69084792\n",
      "Iteration 13, loss = 0.69537785\n",
      "Iteration 14, loss = 0.68770650\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70473102\n",
      "Iteration 2, loss = 0.69382321\n",
      "Iteration 3, loss = 0.68871322\n",
      "Iteration 4, loss = 0.69394315\n",
      "Iteration 5, loss = 0.69065281\n",
      "Iteration 6, loss = 0.69632710\n",
      "Iteration 7, loss = 0.70616300\n",
      "Iteration 8, loss = 0.69370867\n",
      "Iteration 9, loss = 0.68775854\n",
      "Iteration 10, loss = 0.69074185\n",
      "Iteration 11, loss = 0.69364145\n",
      "Iteration 12, loss = 0.69179780\n",
      "Iteration 13, loss = 0.69652256\n",
      "Iteration 14, loss = 0.69061192\n",
      "Iteration 15, loss = 0.69080252\n",
      "Iteration 16, loss = 0.69142125\n",
      "Iteration 17, loss = 0.69734377\n",
      "Iteration 18, loss = 0.69209617\n",
      "Iteration 19, loss = 0.69022307\n",
      "Iteration 20, loss = 0.69371439\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70744709\n",
      "Iteration 2, loss = 0.69422472\n",
      "Iteration 3, loss = 0.69401439\n",
      "Iteration 4, loss = 0.69332387\n",
      "Iteration 5, loss = 0.69165055\n",
      "Iteration 6, loss = 0.69397164\n",
      "Iteration 7, loss = 0.70112233\n",
      "Iteration 8, loss = 0.69490543\n",
      "Iteration 9, loss = 0.69162779\n",
      "Iteration 10, loss = 0.69333788\n",
      "Iteration 11, loss = 0.69612184\n",
      "Iteration 12, loss = 0.69375762\n",
      "Iteration 13, loss = 0.69488084\n",
      "Iteration 14, loss = 0.69252266\n",
      "Iteration 15, loss = 0.69258686\n",
      "Iteration 16, loss = 0.69300643\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69977287\n",
      "Iteration 2, loss = 0.69173183\n",
      "Iteration 3, loss = 0.68766186\n",
      "Iteration 4, loss = 0.69158977\n",
      "Iteration 5, loss = 0.68679058\n",
      "Iteration 6, loss = 0.68754772\n",
      "Iteration 7, loss = 0.69049647\n",
      "Iteration 8, loss = 0.68915777\n",
      "Iteration 9, loss = 0.68911809\n",
      "Iteration 10, loss = 0.69006385\n",
      "Iteration 11, loss = 0.69766520\n",
      "Iteration 12, loss = 0.71600422\n",
      "Iteration 13, loss = 0.69846358\n",
      "Iteration 14, loss = 0.69306890\n",
      "Iteration 15, loss = 0.68788740\n",
      "Iteration 16, loss = 0.68821430\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69765079\n",
      "Iteration 2, loss = 0.69029923\n",
      "Iteration 3, loss = 0.68569927\n",
      "Iteration 4, loss = 0.68972889\n",
      "Iteration 5, loss = 0.68392537\n",
      "Iteration 6, loss = 0.68480663\n",
      "Iteration 7, loss = 0.68757499\n",
      "Iteration 8, loss = 0.68598995\n",
      "Iteration 9, loss = 0.68569263\n",
      "Iteration 10, loss = 0.68953725\n",
      "Iteration 11, loss = 0.68737016\n",
      "Iteration 12, loss = 0.69668588\n",
      "Iteration 13, loss = 0.68579725\n",
      "Iteration 14, loss = 0.68556464\n",
      "Iteration 15, loss = 0.68639041\n",
      "Iteration 16, loss = 0.68512187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70232354\n",
      "Iteration 2, loss = 0.69118085\n",
      "Iteration 3, loss = 0.69123087\n",
      "Iteration 4, loss = 0.69300670\n",
      "Iteration 5, loss = 0.68863310\n",
      "Iteration 6, loss = 0.68731062\n",
      "Iteration 7, loss = 0.69145879\n",
      "Iteration 8, loss = 0.68841393\n",
      "Iteration 9, loss = 0.68874814\n",
      "Iteration 10, loss = 0.69439429\n",
      "Iteration 11, loss = 0.69175394\n",
      "Iteration 12, loss = 0.70081446\n",
      "Iteration 13, loss = 0.69340845\n",
      "Iteration 14, loss = 0.69061855\n",
      "Iteration 15, loss = 0.69224845\n",
      "Iteration 16, loss = 0.68786828\n",
      "Iteration 17, loss = 0.68857729\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70169322\n",
      "Iteration 2, loss = 0.69238823\n",
      "Iteration 3, loss = 0.69116081\n",
      "Iteration 4, loss = 0.69420619\n",
      "Iteration 5, loss = 0.69007652\n",
      "Iteration 6, loss = 0.68931401\n",
      "Iteration 7, loss = 0.68871137\n",
      "Iteration 8, loss = 0.68871152\n",
      "Iteration 9, loss = 0.68943209\n",
      "Iteration 10, loss = 0.70200654\n",
      "Iteration 11, loss = 0.69356119\n",
      "Iteration 12, loss = 0.70352551\n",
      "Iteration 13, loss = 0.69185213\n",
      "Iteration 14, loss = 0.69252921\n",
      "Iteration 15, loss = 0.68846525\n",
      "Iteration 16, loss = 0.68965178\n",
      "Iteration 17, loss = 0.69035750\n",
      "Iteration 18, loss = 0.69206891\n",
      "Iteration 19, loss = 0.69075648\n",
      "Iteration 20, loss = 0.69775241\n",
      "Iteration 21, loss = 0.69092333\n",
      "Iteration 22, loss = 0.69531338\n",
      "Iteration 23, loss = 0.69179237\n",
      "Iteration 24, loss = 0.69259835\n",
      "Iteration 25, loss = 0.69040728\n",
      "Iteration 26, loss = 0.68998708\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70168202\n",
      "Iteration 2, loss = 0.68990473\n",
      "Iteration 3, loss = 0.68991596\n",
      "Iteration 4, loss = 0.69473973\n",
      "Iteration 5, loss = 0.68792318\n",
      "Iteration 6, loss = 0.68879514\n",
      "Iteration 7, loss = 0.68782313\n",
      "Iteration 8, loss = 0.68918832\n",
      "Iteration 9, loss = 0.69202940\n",
      "Iteration 10, loss = 0.70245903\n",
      "Iteration 11, loss = 0.69917061\n",
      "Iteration 12, loss = 0.70607651\n",
      "Iteration 13, loss = 0.69264119\n",
      "Iteration 14, loss = 0.68921789\n",
      "Iteration 15, loss = 0.68998496\n",
      "Iteration 16, loss = 0.68860047\n",
      "Iteration 17, loss = 0.68877302\n",
      "Iteration 18, loss = 0.69069055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.520213963963964\n",
      "Accuracy for ML is 52.021396396396405%\n"
     ]
    }
   ],
   "source": [
    "preprocessing_features = preprocessing.scale(X)\n",
    "preprocessing_features\n",
    "\n",
    "my_ANN = MLPClassifier(hidden_layer_sizes=(30,), activation= 'logistic', \n",
    "                       solver='adam', alpha=1, random_state=1, \n",
    "                       learning_rate_init = 0.02, verbose=True)\n",
    "\n",
    "accuracy_list = cross_val_score(my_ANN, preprocessing_features, y, cv=10, scoring='accuracy')\n",
    "\n",
    "accuracy_cv_mean = accuracy_list.mean()\n",
    "\n",
    "print(accuracy_cv_mean)\n",
    "print(\"Accuracy for ML is \" + str((accuracy_cv_mean * 100)) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
